<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="Dubers Blog" />



  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="Dubers Blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Duburs Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Duburs Blog">
<meta property="og:description" content="Dubers Blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Duburs Blog">
<meta name="twitter:description" content="Dubers Blog">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>

  <title> Duburs Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  

  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?de04554947ef5ddb1d9dd95ef6598c19";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">Duburs Blog</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分類
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            <i class="menu-item-icon icon-next-about"></i> <br />
            關於
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            歸檔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            標籤
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/24/Caffe-源码阅读笔记一（基本代码架构）/" itemprop="url">
                Caffe 源码阅读笔记一（基本代码框架）
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-24T22:11:07+08:00" content="2015-08-24">
            2015-08-24
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/caffe/" itemprop="url" rel="index">
                  <span itemprop="name">caffe</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/24/Caffe-源码阅读笔记一（基本代码架构）/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/24/Caffe-源码阅读笔记一（基本代码架构）/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>在开始阅读Blob之前，需要预先了解Caffe的架构与protobuf，否则看代码的时候容易一头雾水</p>
<h3 id="caffe基本架构">Caffe基本架构</h3>
<p>对于任何一个网络而言，Caffe主要用4个类来构造网络：</p>
<ul>
<li>BLob:存储数据，blob既可以存储网络的权重，在进行训练时修改的其实就是每个blob实例</li>
<li>Layer:构成网络的基础单元，无论是卷积层，pooling层还是全连接层等等都是从这个类派生出来的</li>
<li>Net:定义一个网络的架构，当我们把网络中每一层都建立完之后，通过Net类来搭建整个网络</li>
<li>Solver:用来训练网络的类，例如SGD就在这部分实现</li>
</ul>
<h3 id="protobuf">Protobuf</h3>
<p>在理清基本架构之后可能有人会问：一个网络需要那么多参数，且每个参数的数据类型不尽相同，如何解决这个问题呢？</p>
<p>Protobuf就是用来解决此类问题的，我们只需要像写配置文件一样把网络中每一层的参数设置好(当然配置文件必须按照一定的格式)，protobuf会自动生成代码，用来解析“配置文件”中的参数结构体，代码中包括了对参数的设置、读取和序列化等等操作 另外当网络训练完成之后，存储时也使用了protobuf进行处理，把Net转化为多个Message对象进行存储</p>
<p>Caffe代码中参数初始化和参数操作都是基于protobuf完成，只需要定义网络参数，不再需要考虑如何把参数传递给应用程序的问题了，protobuf会帮你完成～</p>
<h3 id="blob">Blob</h3>
<p>Blob本身其实没有特别多的内容，作为Caffe中数据的基础单元，我们可以简单地吧Blob理解成一个容器，里面存储的是多维向量及其相关的信息</p>
<p>首先看Blob.hpp:</p>
<pre><code>Blob()
 : data_(), diff_(), count_(0), capacity_(0) {}
   </code></pre>
<p>构造函数中初始化了4个对象，data_表示Blob中的数据，diff_表示反向传播时的误差，count_表示数据当前的维度(因为可以reshape)，而capacity_表示数据最大的维度</p>
<p>接下来在Blob.cpp里主要包含了实例的初始化和reshape函数，值得注意的是如果blob中存储的是网络的参数(如全连接层和卷积层的kernel)，那么Blob提供update函数用来更新自己的参数。</p>
<pre><code>void Blob&lt;Dtype&gt;::Update() {                                                                                                                                  
  // We will perform update based on where the data is located.                                                                                               
  switch (data_-&gt;head()) {                                                                                                                                    
  case SyncedMemory::HEAD_AT_CPU:                                                                                                                             
    // perform computation on CPU                                                                                                                             
    caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),                                                                                                                      
      static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),                                                                                                         
      static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));                                                                                                      
    break;                                                                                                                                                    
  case SyncedMemory::HEAD_AT_GPU:                                                                                                                             
  case SyncedMemory::SYNCED:                                                                                                                                  
  #ifndef CPU_ONLY                                                                                                                                              
  // perform computation on GPU                                                                                                                             
  caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),                                                                                                                  
      static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),                                                                                                         
      static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));                                                                                                      
  #else                                                                                                                                                         
  NO_GPU;                                                                                                                                                   
  #endif                                                                                                                                                        
  break;                                                                                                                                                    
  default:                                                                                                                                                    
    LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;                                                                                                               
  }                                                                                                                                                           
}                            </code></pre>
<p>代码中包含了GPU和CPU两种实现，其实核心就是caffe_gpu_axpy那一句更新参数</p>
<h3 id="layer">Layer</h3>
<p>对Blob有了一定了解之后可以看Layer了，事实上Layer是caffe架构中内容最多的部分，我们在写配置文件的时候其实都是在组合layer构成一个网络，很多运算操作在caffe中都是以layer的形式存在的，如argmax和elementwise运算等等.许多对caffe的扩展其实也是继承Layer层重新定义了具有新功能的层</p>
<p>layer的工作模式类似数学中的函数概念，给定一个输入(bottom blobs),layer内部完成自己的功能，返回一个输出(top blobs)</p>
<p>caffe中的layer分两种，common layer和vision layer: - data_layers.hpp中声明了神经网络与输入数据之间的交互层，如导入/导出hdf5数据，从图像中导入数据等等 - common_layers.hpp中声明了许多常用的包含基础功能(flatten, softmax等)的层 - vision_layers.hpp中主要包含对针对图像处理的层(convolutional, pooling等) - neuron_layers.hpp中主要包含与神经元的定义与操作相关的层(dropout,ReLu等) - loss_layers.hpp中主要包含了计算网络Loss的层，除了我们常见的softmax loss之外还包括了比较常用多euclidean loss, hinge loss等等 看了下vision layer中居然还包含了SPPnet这种处理image scale的层，除此之外还有deconvolution layer,确实是很给力啊</p>
<p>caffe还提供了layer_factory供其他人快速实现自己的layer</p>
<p>所有的Layer层都包含Forward和Backward的函数，这两个函数包含了整个layer的计算功能，对于开发者而言如果想实现自己的layer，其实主要就是完成这两个函数逻辑</p>
<p>具体的某一种Layer的代码解析不在本文的范畴里，以后会另开blog详细描述常用的layer代码实现</p>
<h3 id="net">Net</h3>
<p>Net中包含了对整个网络的操作实现，在看源代码之前笔者只想到了设置learning_rate、 weight_decay和迭代次数等等功能，以及控制Forward和Backward等，然而事实上caffe提供了许多非常有用的函数，如在RCNN training中提到的share weights功能，使得你可以直接使用预先训练好的网络参数进行初始化，这也是为什么在caffe的example中我们可以直接通过命令行对imagenet训练出来的网络进行fine－tuning的原因</p>
<h3 id="solver">Solver</h3>
<p>相对而言solver的实现比较简单，创建solver对象时我们只需要关注优化算法就可以了，caffe目前提供三种梯度下降的方法： - SGD(momentum) - NesterRov - Adagrad</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/23/CNN经典论文集/" itemprop="url">
                CNN经典论文集
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-23T20:27:23+08:00" content="2015-08-23">
            2015-08-23
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/23/CNN经典论文集/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/23/CNN经典论文集/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>这篇日志主要用来整合近几年在ImageNet竞赛中获得较高成绩的CNN网络相关论文的笔记，因为思想都是基于CNN，在实际训练网络或者网络结构上有创新的地方，因此就统一写成一篇文档了，实验结果部分就省略了，主要集中在文章亮点的地方，一共有4篇相关的论文(Alexnet, GoogleNet, VGGnet和OverFeat)，会保持长期更新。</p>
<p>caffe的example中给出了Alexnet和GoogleNet的模型，熟悉caffe的话可以直接看对应的depoly.prototxt,比较直观</p>
<h2 id="alexnet">AlexNet</h2>
<h3 id="architecture">Architecture</h3>
<p>Alexnet是ILSVRC2012的冠军，网络结构比较深但是很有规律，关于这篇论文有许多在实际train网络时候的一些比较有用的trick，AlexNet的训练过程如下：</p>
<ul>
<li>rescale the image such that the shorter side is of length 256, and crop the centeral 256*256 patch from the resulting image 先rescale后crop的方式处理图像scale的问题</li>
<li>5个卷积层3个pooling 层，且神经元的激活函数使用的是ReLu，降低计算量</li>
<li>Training on multiple GPUs，现在看来GPU并不是非常好，但是多GPU并行计算</li>
<li><p><strong><em>LRN（Local Response Normalization） Normalization</em></strong> 笔者认为这是本文的亮点，虽然ReLu不需要输入normalization但是引入LRN可以提升1.4%至1.2%的准确率，简单来说就是对ReLu层的输出进行归一化，归一化公式如下： <span class="math">\[b_{x,y}^{i} = a_{x,y}^{i}/(k+\alpha\sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(a_{x,y}^j)^2)\]</span> 其中i和j表示第i/j个卷积的kernel，而a表示ReLu的输出，b表示归一化后的结果</p>
注：caffe的LRN层实现中提供两种Normalization的方式：通道内归一化和通道间归一化，这个在caffe源码笔记中会详细讲解</li>
<li><p>Overlapping Pooling 传统的pooling方式将feature map分成独立的grid每个grid进行pooling，而本文中采用滑动窗口的方式，每次滑动的stride小于窗口的大小，实验证明，这样的方法能提升0.4%－0.3％的准确率</p></li>
</ul>
<p>整体网络结构直接上图：<img src="/img/CNN/AlexNet.jpeg" alt="AlexNet"></p>
<h3 id="training-details">Training Details</h3>
<h4 id="data-augmentation">Data Augmentation</h4>
<p>笔者认为这一部分内容在工业界相当有用，Deep learning需要大量的数据，而当数据集不够充足的时候如何做好数据增量是十分关键的，同时还能减少overfitting</p>
<p>文中介绍了两种方法： - generating image translations and horizontal reflections(截取部分图像和镜像翻转) - alter the intensities of the RGB channels(借助PCA改变RGB通道的像素值)</p>
<h4 id="dropout">Dropout</h4>
<p>Dropout有专门的文献介绍，简单来说就是每次Forward的时候神经元以某概率输出0，这样在BP的时候该神经元的权重也不会更改</p>
<h2 id="googlenet">GoogleNet</h2>
<p>GoogleNet以及其复杂的网络结构而闻名，ILSVRC2014的冠军，一共22层的网络看上去复杂但依旧有规律可循，文章中主要的亮点在于Inception结构</p>
<h3 id="motivation">Motivation</h3>
<p>单纯加深网络会导致过拟合问题出现，且由于90%的计算量集中在最后的全连接层，为了降低计算量，文中提出了将全连接层替换为稀疏连接层。</p>
<h3 id="architecture-1">Architecture</h3>
<p>Inception Module是构成整个GoogleNet的关键组件，其主要思想就是用密集的组件来逼近稀疏的卷积网络结构</p>
<p>Naive Inception结构如下图，假设每个Inception视作一个部件级联起来，Inception在接收上一层的结果后，用不同尺寸的卷积层(1*1,3*3,5*5)和pooling层并行计算结果，将每层的结果合并起来，最后经过一个滤波器进行分组</p>
<p>但是会带来一个问题：即使是一个合适数量的卷积，也会因为大量的滤波而变得特别expensive。经过pooling层输出的合并，最终可能会导致数量级增大不可避免。处理效率不高导致计算崩溃。</p>
<p>因此改进方案是：在需要大量计算的地方进行降维。压缩信息以聚合。1*1卷积不仅用来降维，还用来引入非线性特性。</p>
<h3 id="googlenet-1">GoogleNet</h3>
<div class="figure">
<img src="/img/CNN/googlenet.jpg" alt="GoogleNet">
<p class="caption">GoogleNet</p>
</div>
<p>比较有趣的地方是网络中有三个softmax分类器，文中作者提到对于浅层网络而言，中间网络产生的特征非常有辨识力，在这些层中增加一些额外的分类器能增加BP的梯度同时提供额外的正则化，每3个Inception module会增加一个Softmax分类器。在训练过程中，损失会根据权重叠加，而在测试时丢弃。</p>
<p>Training的时候使用了AlexNet中的trick</p>
<h3 id="insights">Insights</h3>
<ul>
<li>approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural net- works for computer vision.(虽然增加了计算量但是能显著提高分类的准确率)</li>
<li>For both classification and detection, it is expected that similar quality of result can be achieved by much more ex- pensive non-Inception-type networks of similar depth and width.(没有使用bounding box regression,说明Inception同样适用于detection和localization的task)</li>
</ul>
<h2 id="vgg-net">VGG net</h2>
<p>VGG Net是ILSCVRC2014 classification的第二名，localization的第一名，网络结构上比其他网络都要来得复杂(实验过程中卷积层最多有16个)，但是文章中对如何构造复杂网络和训练网络的描述非常详尽，相信对research会有很多帮助</p>
<h3 id="network-configuration">Network Configuration</h3>
<h3 id="training">Training</h3>
<p>网络的training借鉴了AlexNet在训练中使用的方法，同样是momentum SGD＋dropout，处理图像尺寸的方法相同，做数据增量的方法也相同，但是由于一共有5个网络需要训练，且网络深度依次递增，因此在参数初始化时作者提出：</p>
<p>最简单的网络使用随机初始化，而后面的网络中最前面的4个卷积层与最后3个全连接层参数用先前网络的参数初始化,其他层使用随机初始化，这种<strong><em>首先训练简单网络，随后使用其参数来初始化复杂网络的训练方法是一种非常合理且高效的方式</em></strong></p>
<p>值得一提的是文中对比了7*7的卷积层与三个3*3的卷积层之间的对比，相比较而言多个小的卷积层级联能在引入更多的linear rectify之外同时还能降低卷积层参数的数量。</p>
<p>整个训练过程如下：</p>
<ul>
<li>Classification
<ul>
<li>设计结构复杂度依次递增的网络</li>
<li>在Image size的处理上，作者提出single-scale和multi－scale的方法, single-scale的方法与AlexNet相同，先resize后crop，multi－scale的方法类似于随机采样，每次rescale之前在一个固定区间内（如[256,512]）选取一个scale，这样每个图像的尺寸就不同了，但是怎么处理feature长度不同的问题作者似乎没有说明</li>
<li>每次训练新的网络之前用之前网络的参数进行初始化以加快收敛速度</li>
</ul></li>
<li>Localization
<ul>
<li>Training Localization的方法与classification相同，只不过用的是Euclidean Loss</li>
</ul></li>
</ul>
<h3 id="testing">Testing</h3>
<p>在testing阶段，网络接收图像数据之前首先需要把图像rescale到固定大小，作者称之为test scale，之后首先通过fc层计算，我们把fc层看作一个卷积窗口为1*1的卷积层，得到的feature map维度与object类别相同。</p>
<p>这样做的好处是对比首先crop再通过卷积计算feature map的做法，直接通过fc层计算可以减少许多重复的卷积计算量，crop出来的图像之间必然会有重叠的部分，这些重叠部分增加了测试时卷积的计算量，但是笔者认为既然在训练时使用crop来做数据增量，测试时用crop可以增加网络输出的置信度（每个crop输出分类结果，这个结果甚至可以用来做类似bagging的训练），如果面对实时性要求比较高的情况，对整张图片应用fc层是比较可行的方法。</p>
<h2 id="overfeat">OverFeat</h2>
<p>OverFeat最出彩的地方在于使用一个CNN解决Classfication、Localization和Detection的任务，以及Sliding Window和CNN的结合，每个任务使用的神经网络不同之处只在最后的分类器，大牛Yann LeCun的文章</p>
<p>网络结构的前五层与Alexnet相同，Alexnet的网络结构已经成为feature extraction的经典结构了,如下图所示： <img src="/img/CNN/OverFeat.png" alt="OverFeat"></p>
<h3 id="multiscale-training">Multiscale training</h3>
<p>训练时大多数人都会碰到Multiscale的问题，本文中作者提出在pooling时引入偏移量的方法，具体过程如下图 <img src="/img/CNN/Pooling.png" alt="Pooling"> 按照笔者理解，原始的3*3pooling的起始位置为(0,1,2),其实就是使用stride步长为1的pooling层，之后按照原始stride进行分组，得到多个分组的pooling向量之后使用滑动窗口的方式得到feature map，重新将原先分组的结果合并成一个向量作为整张图片的feature map,无论图像的尺寸多大，使用stride为1的pooling层能最大程度上保持原始信息，并且分组之后每一个feature map向量长度保持一致。</p>
<h3 id="sliding-window">Sliding Window</h3>
<p>训练CNN时要求输入图像的大小固定，但是如果输入的图像尺寸比原先大怎么办呢？对于一个训练好的CNN来说，卷积核的大小和数量以及每一层feature map的个数是固定的，但是feature map的大小是可以改变的。</p>
<p>当测试样本和训练样本大小相同时，CNN最后一层的每一个节点分别输出一个0~1的实数，代表测试样本属于某一类的概率；当测试样本比训练样本大时，CNN最后一层每一个节点的输出为一个矩阵，矩阵中的每一个元素表示对应的图像块属于某一类的概率，其结果相当于通过滑动窗口从图像中进行采样，然后分别对采样到的图像块进行操作 <img src="/img/CNN/SlidingWindow.png" alt="Sliding"></p>
<h3 id="localization">Localization</h3>
<p>将原先classification网络的最后一层softmax替换为Bounding box回归: 最后一个pooling层输出的feature map通过两个全连接层映射到一个输出层，输出被看作一个由4维向量构成的矩阵，每个向量对应了原始图像某一块区域中的object的bounding box的位置，之后开始合并各个区域中的bounding box直到两个bounding box之间的match score足够大。</p>
<p>值得注意的是这个regression网络对于每一个object类别都会输出一个bounding box，前面一节中提到的classification的网络会输出图像分类的概率值，于是每一类的bounding box都会对应一个置信度，在合并之后网络会输出一个置信度，这个置信度就是原先所有class置信度的最大值，因此完成合并之后网络的输出是一个带有最大confidence的bounding box</p>
<h3 id="detection">Detection</h3>
<p>物体检测的方法其实就是结合了classification以及localization，但是文中没有给出具体的训练细节，笔者认为与Localization非常相似，因为直观上只要根据置信度删选Localization输出的bounding box，之后取每一个图像块中置信度最大bounding box就可以了，可以看出相对于RCNN的做法，OverFeat比较暴力一些，对每块图像区域都会输出1000类的预测结果，其计算量想必也不小</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/16/Faster RCNN论文笔记/" itemprop="url">
                Faster RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-16T17:10:30+08:00" content="2015-08-16">
            2015-08-16
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/16/Faster RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/16/Faster RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>本笔记中会多次提到Fast RCNN的架构，对Fast RCNN的概念与架构有疑问的可以参考</p>
<p><a href="http://arxiv.org/pdf/1504.08083v1.pdf" target="_blank" rel="external">Fast RCNN</a></p>
<p>或者我的Fast RCNN笔记</p>
<p><a href="http://dubur.github.io/2015/08/12/Fast%20RCNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" target="_blank" rel="external">Notes on Fast RCNN</a></p>
<p>MSR的研究员似乎对Fast RCNN的成果还不够满意，从RCNN到Fast RCNN，所有的detection任务都使用了selective search来提取region proposal，因此诞生了用神经网络来提取region proposal的方法，进一步提升检测速度的同时还提升了检测的准确率，然而代码现在还没有公开。</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>Fast RCNN ignores the time spent on region proposals</li>
<li>Region proposal methods used in research are implemented on the CPU</li>
<li><strong><em>The feature maps can also be used for generating region proposals</em></strong> 简单说就是提取proposal的步骤太花时间，而且作者希望尽可能用GPU完成所有的计算。</li>
</ul>
<h2 id="region-proposal-networks-rpn">Region Proposal Networks (RPN)</h2>
<h3 id="basic-notions">Basic Notions</h3>
<ul>
<li>Fully-convolutional network for generating region proposals</li>
<li>Share computation of convolutions with start-of-art object detection networks</li>
</ul>
<h3 id="architecture">Architecture</h3>
<ul>
<li>RPN与卷积层共享权重，也就是说RPN的输入就是最后一个卷积层的输出的feature map，得到输入后使用滑动窗口的方式得到更低维的向量</li>
<li>将得到的向量输入到两个并联的全连接层</li>
</ul>
<ol style="list-style-type: decimal">
<li>box-regression layer (bounding box regressor)</li>
<li>box-classification layer (objectness) 类似于Fast RCNN的RoI pooling层后面的架构，feature用来训练两个分类器，一个用来判断这个RoI是否包含object，另一个用来做bounding box回归(即给定一个anchor判断bounding box的位置)</li>
</ol>
<ul>
<li><p>每个滑动窗口中心记作一个anchor，对应9种variant，相当于对于每一个feature map用9种sliding window计算vector，后面的输出也变成9倍，这么做是为了对图像的transformation有更好的鲁棒性 <img src="/img/Faster_RCNN/RPN.png" alt="RPN"></p></li>
<li><p>Loss Function的定义与Fast RCNN类似</p></li>
</ul>
<!--<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>-->
<p><span class="math">\[ L(p\_{i},t\_{i}) = L\_{cls}(p\_{i},{p\_{i}}^{*}) + \lambda{p\_{i}}^{*}L\_{reg}(t\_{i},{t\_{i}}^{*}) \]</span></p>
<p>由两部分构成：regression的Loss和classification的Loss，通过<span class="math">\(\lambda\)</span>控制两类Loss的比重 公式中<span class="math">\(p_{i}\)</span>表示一个region中包含object的概率，<span class="math">\({p_{i}}^{*}\)</span>表示ground truth， 是一个0-1指示器，当某个anchor被标注为含有一个object时它的值才是1，否则为0, <span class="math">\(L_{cls}(p_{i},{p_{i}}^{*})\)</span>的定义方式与Fast RCNN中相同</p>
<h2 id="training-rpn">Training RPN</h2>
<ul>
<li>mini batch构成</li>
</ul>
<ol style="list-style-type: decimal">
<li>随机选取一张图片的256个anchor，正负样本为1:1</li>
<li>用高斯分布初始化权重</li>
</ol>
<ul>
<li>Trainging的四个步骤 这里引用一下原文:</li>
</ul>
<ol style="list-style-type: decimal">
<li>First, train the RPN is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task.</li>
<li>In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection network is also initialized by the ImageNet-pre-trained model.</li>
<li>In the third step, we use the detector network to initialize RPN training, but we fix the shared conv layers and only fine-tune the layers unique to RPN.</li>
<li>Now the two networks share conv layers. Finally, keeping the shared conv layers fixed, we fine-tune the fc layers of the Fast R-CNN.</li>
</ol>
<p>总体而言训练方式在训练Fast RCNN的基础上做了改进，Fast RCNN输入的region proposal由RPN提供，之后用Fast RCNN的权重重新初始化RPN的参数，做到权重共享，保持这些权重不变，只对RPN中的几层进行微调，最后再对全连接层进行微调。</p>
<p>这样做的好处显而易见，实时检测时，前面卷积层提取feature map直接送给RPN， Fast RCNN等待RPN计算proposal，用GPU计算proposal速度很快，与Fast RCNN不同的是不需要selective search，提取proposal的步骤与detection合并，在原先的卷积层与RoI pooling层之间加入了一个RPN提取proposal，之后Fast RCNN的全连接层利用RPN的输出向量进行detection</p>
<p>笔者认为从RCNN到这里的Faster RCNN，有一点bounding box regression和object classification的joint learning的味道了。</p>
<p>最后RPN结合Fast RCNN可以做到单纯用神经网络进行object detection，不依靠selective search这样的low-level feature的方法，完全依靠deep learning进行图像的理解。</p>
<p>不过有时间还是要把selective search也看一下，毕竟这篇论文有很高的引用量。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/12/RCNN论文笔记/" itemprop="url">
                RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-12T16:17:30+08:00" content="2015-08-12">
            2015-08-12
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/12/RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/12/RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>RCNN本身概念比较简单，但是论文中提到了许多object detection的经典方法非常有用，这些内容绝大部分在附录中，在阅读其他论文时相信会有很大帮助。</p>
<h2 id="basic-notions">Basic Notions</h2>
<ul>
<li>Pipeline method</li>
</ul>
<ol style="list-style-type: decimal">
<li>提取独立于物体类别的region proposal</li>
<li>使用CNN做特征提取</li>
<li>调整CNN参数用于从候选的proposal中获取包含object的proposal(domain-specific fine-tuning)</li>
<li>SVM进行分类</li>
</ol>
<h2 id="rcnn-for-object-detection">RCNN for Object Detection</h2>
<h3 id="testing">Testing</h3>
<ul>
<li>Selective search 提取region proposal(非本文重点)</li>
<li>使用已经训练好的CNN(dataset: ILSVRC2012 classification caffe implementation) 提取proposal的特征，为了将输入标准化，强制将每个proposal resize到227*227</li>
<li>CNN提取的特征输入到SVM中进行分类</li>
</ul>
<h3 id="training">Training</h3>
<p>按照个人理解，为了让CNN有detection的能力，我们需要让网络具备分辨哪些proposal可能包含object，因为selective search选取的许多proposal其实没有object或者仅包含了object的一小部分。</p>
<p>因此文中加了一步对CNN参数的调整(domain-specific fine-tuning),主要目的就是筛选proposal</p>
<p>在实验部分作者也进行了对比，结果证明使用了fine-tuning比不使用要高8%的mAP</p>
<ul>
<li>Supervised pre-training</li>
</ul>
<ol style="list-style-type: decimal">
<li>CNN on ILSVRC2012 with 2.2% error rate</li>
<li>training set: images with image-level annotations, <strong><em>no bounding boxes</em></strong></li>
</ol>
<ul>
<li>Domain specific fine-tuning</li>
</ul>
<ol style="list-style-type: decimal">
<li>N+1 classification: N object classes and 1 background</li>
<li>Use warped region proposals for tuning</li>
<li>Positive training examples: region proposals with &gt;0.5IoU overlap(the rest as negative)</li>
</ol>
<ul>
<li>Object category classification</li>
</ul>
<ol style="list-style-type: decimal">
<li>Positive examples: ground-truth boxes, Negative: region proposals with &lt;0.3IoU overlap</li>
<li>Trade-off between SVM and Softmax</li>
<li>Different definition for training sets between fine-tuning and classification</li>
</ol>
<p>这里作者提了两个问题 (在附录里均有解答):</p>
<ol style="list-style-type: decimal">
<li>为什么在第二步调整CNN参数使用的训练集和训练SVM分类器的训练集不同(尤其是threshold的选择不一样)?</li>
<li>为什么不使用softmax分类器而选择SVM?</li>
</ol>
<h2 id="appendix">Appendix</h2>
<ul>
<li>Object proposal transformations 由于CNN输入是227*227的正方形图像，因此需要对proposal进行缩放处理，文中提出了3种不同的处理方式：</li>
</ul>
<ol style="list-style-type: decimal">
<li>tightest square with context(等比例缩放proposal在原始图像中的bounding square)</li>
<li>tightest square without context(等比例缩放proposal在原始图像中的bounding square,但要去掉在bounding square中而不在原始proposal中的部分)</li>
<li>强制缩放proposal 3种变形的方式如图中所示:<img src="/img/RCNN/transformation.png" alt="transformation"></li>
</ol>
<ol style="list-style-type: upper-alpha">
<li>origin proposal (B) tightest square with context (C) tightest square without context (D) warp</li>
</ol>
<ul>
<li>Positive vs negative examples and softmax</li>
</ul>
<ol style="list-style-type: decimal">
<li>用于fine－tuning的数据集过少，因此将降低了positive example的要求，只要IoU大于0.5就认为可能有object在这个proposal里面，作者同时指出这样能够防止overfitting</li>
<li>使用softmax比SVM降低3.3%的mAP，主要原因可能在于fine-tuning的训练集不那么强调box的位置，且SVM使用的nagative sample更具有区分度</li>
</ol>
<ul>
<li>Bounding-box 回归</li>
</ul>
<ol style="list-style-type: decimal">
<li>使用最后一个pool层的输出训练线性回归模型来预测detection window</li>
<li>实验结果表明使用BB regression效果更优</li>
</ol>
<h2 id="insights">Insights</h2>
<ul>
<li>Most feature information is extracted by the convolutional layers</li>
<li>The structure of CNN really matters(nearly 7~8% mAP)</li>
</ul>
<p>下面两点是这篇文章的核心：</p>
<ul>
<li>High-capacity CNN to bottom-up region proposals in order to localize and segment objects(CNN用于detection)</li>
<li>Effective method: supervised pretraining (auxiliary data eg. classification)/fine-tuning(scarce data eg. detection)(fine-tuning的重要性不言而喻)</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/12/Fast RCNN论文笔记/" itemprop="url">
                Fast RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-12T16:17:30+08:00" content="2015-08-12">
            2015-08-12
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/12/Fast RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/12/Fast RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>Fast RCNN在RCNN的基础上修改了网络结构，同时加快了训练速度并且降低了训练成本，从工程角度说RCNN提出了思路而Fast RCNN的实用性更强</p>
<h2 id="motivation-drawbacks-of-rcnn">Motivation (Drawbacks of RCNN)</h2>
<ul>
<li>RCNN的训练方式按照一个一个阶段分的比较开，</li>
<li>训练时RCNN会将每个proposal的特征向量存储在硬盘上，不仅占用存储空间，时间上也因为读写磁盘而变得很慢</li>
<li>在进行detection测试时耗时长</li>
</ul>
<h2 id="fast-rcnn-training">Fast RCNN Training</h2>
<p>文章的亮点在于对原来网络结构的改进，这里就不得不膜拜大神了</p>
<ul>
<li>RoI Pooling Layer 按照作者所述，该层的思想类似于SPPnet，我们假设最后一个卷积层得到输入N个feature map(这些feature map包含原始图像的所有信息)，而我们只想提取感兴趣的部分，因此这就需要引入RoI了</li>
</ul>
<p>将这N个feature map和R个region proposal输入到RoI pooling层中，根据RoI中指定的区域对原始的feature map进行pooling得到这R个region proposal的N个feature map</p>
<p>笔者觉得其实和SPPnet没有多大的关系……由于各个RoI的大小不一样，而为了使得每个RoI pooling出来的feature长度相同，采用不同大小的pooling窗口而已</p>
<ul>
<li>对原始CNN网络结构的修改 修改主要有3处</li>
</ul>
<ol style="list-style-type: decimal">
<li>将最后一个max pooling层替换为RoI pooling层</li>
<li>将最后一个全连接层和softmax层替换为两个同并列的层
<ol style="list-style-type: decimal">
<li>K+1个class的分类器</li>
<li>Bounding box回归(Localization)</li>
</ol></li>
<li>修改网络的输入使其接收N个image和R个RoI 最后网络结构如图所示：<img src="/img/Fast_RCNN/fast-rcnn.jpeg" alt="fast-rcnn"></li>
</ol>
<p>核心在于使用RoI pooling层直接提取原始feature map中属于RoI的那一部分</p>
<ul>
<li><p>Fine-tuning微调</p>
<p>网络结构变化带来的是训练方式的变化，作者在现有的CNN分类网络上进行了微调，类似于RCNN中的domain-specific fine-tuning,只不过这次是针对网络结构而调整训练方式</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Multi-task Loss</p>
<p>简单来说就是将分类的Loss和bounding box回归的Loss做了加权和</p>
<p><span class="math">\[ L(p,k^*,t,t^*) = L_{cls}(p,k^*) + \lambda[k^*\ &gt;\ 1]L_{loc}(t,t^*) \]</span></p>
<p><span class="math">\[ smooth_{L_1}(x) = \begin{cases}0.5x^2 \ &amp;if \ |x| &lt; 1
 \cr |x|-0.5 \ &amp;otherwise \end{cases} \]</span></p>
<p>第一部分是softmax分类器的Loss，第二部分是regression的Loss，值得注意的是第二部分有个指示函数，说明不会计算那些background类型的proposal的locaolization误差</p></li>
<li><p>mini-batch sampling</p>
<p>每一次SGD的batch使用2个image和128个RoI，每个batch中RoI的构成: 1.25% ground truth中的bounding box IoU&gt;0.5, 2.75% IoU &lt; 0.5, 看作background example</p></li>
<li><p>BP through RoI pooling layer</p>
<p>在进行反向传播时RoI pooling layer有些许不同, 参与pooling的不再是整幅图像而是可能有重叠的RoI区域，计算公式如下:</p>
<p><span class="math">\[ {\partial L \over \partial x} = \sum_{r\in R} \sum_{y\in r} [y\ pooled\ x] {\partial L \over \partial y} \]</span></p>
<p>y代表pooling后的值, 反向传播的思想是要将每个y的偏导传递给所有参与pooling的变量x，这也就是指示函数 [y pooled x] 的含义，在这里x代表的是原来RoI中的元素</p></li>
</ol>
<ul>
<li>Scale invariance 如何处理输入图像尺寸不一的问题？ 作者给出了两种方案:</li>
<li>brute force 强制把图像resize到同一尺寸下</li>
<li>image pyramids 利用图像金字塔，把图片resize成不同的大小，选取其中最接近optimal scale（也就是227*227）的大小用作训练。</li>
</ul>
<h2 id="fast-rcnn-detection">Fast RCNN Detection</h2>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<ul>
<li>Truncated SVD 全连接层的计算时间太长，占用了整个前向传播的一半时间，因此在进行矩阵乘法时，作者采用SVD将权重矩阵<span class="math">\(W (u \times v)\)</span>分解为三个矩阵的乘积：</li>
</ul>
<p><span class="math">\[ W \approx U \Sigma_tV^T \]</span></p>
<p>SVD分解后的U和V都是对称矩阵 最后一个全连接层被分解为两个子层:第一层的权重矩阵为$ _tV^T <span class="math">\(,第二层的权重矩阵为\)</span>U<span class="math">\(,因此整个参数的个数由\)</span>uv<span class="math">\(变成了\)</span>t(u+v)$,当t比较小(意味着原参数矩阵的特征值少)的时候计算时间就会大大减少 <img src="/img/Fast_RCNN/training_time.png" alt="training_time"></p>
<h2 id="experiment-insights">Experiment Insights</h2>
<ul>
<li>卷积层和全连接层一样需要fine-tuning &gt; decrease mAP from 66.9% to 61.4%</li>
</ul>
<p>，且浅层的卷积层没有微调的必要。</p>
<ul>
<li>首先用classification loss训练，之后加入bbox regression训练效果更好 &gt; improvement ranges from +0.8 to +1.1 mAP points</li>
</ul>
<p>(那为什么前面在写网络结构时还给出sibling layer的训练方式…)</p>
<ul>
<li>trade-off: speed &gt; performance improvement brought by multi-scale</li>
<li>More data brings higher mAP</li>
<li>Softmax slightly outperforms SVM</li>
<li>Sparse proposals(selective search) improve detection quality and dense proposals(sliding window) free the heavy running cost.</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/06/theano-and-caffe-installation/" itemprop="url">
                Ubuntu14.04 安装 theano 和 caffe
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-06T23:34:01+08:00" content="2015-08-06">
            2015-08-06
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Instructions/" itemprop="url" rel="index">
                  <span itemprop="name">Instructions</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/06/theano-and-caffe-installation/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/06/theano-and-caffe-installation/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>说明:安装时为了支持python版本，且为了安装方便，最好首先安装anaconda，anaconda安装起来方便并且集成了许多科学计算库，主要为了配合theano使用</p>
<p>如果想安装opencv、添加caffe matlab等等最好先看教程中相关部分最后再编译caffe比较保险</p>
<h2 id="安装caffe">安装caffe</h2>
<h3 id="安装build-essentials">1.安装build-essentials</h3>
<p>安装开发所需要的基本包（一般装完系统就有） sudo apt-get install build-essential</p>
<h3 id="安装nvidia驱动">2.安装NVIDIA驱动</h3>
<h4 id="退出图形界面">2.1 退出图形界面</h4>
<pre><code>1）由于有的带有gpu的电脑在启动时默认使用独显作为主要显示设备，因此我们需要将bios设置改为使用集显作为显示设备
2）进入ubuntu，按ctrl＋alt＋F1进入tty，登陆tty后输入：
    sudo service lightdm stop
其他desktop manager也需要关闭</code></pre>
<h4 id="安装驱动">2.2安装驱动</h4>
<p>准备工作：</p>
<p>1）Verify the system has a CUDA-capable GPU.</p>
<p>控制台输入以下命令：</p>
<pre><code>lspci | grep -i nvidia</code></pre>
<p>如果列出了当前NIVDIA显卡的信息则说明电脑的GPU事CUDA-capable的</p>
<p>2）Verify the system is running a supported version of Linux.</p>
<p>控制台输入以下命令:</p>
<pre><code>uname -m &amp;&amp; cat /etc/*release</code></pre>
<p>输出的结果中如果显示x86_64则说明电脑是x86架构的，在下载驱动安装包时选择对应的包即可</p>
<p>3）Verify the system has gcc installed.</p>
<p>控制台输入:</p>
<pre><code>gcc --version</code></pre>
<p>确认gcc版本正确</p>
<p>4）Download the NVIDIA CUDA Toolkit.</p>
<p>从NVIDIA官网上下载对应的驱动安装包 下载地址:</p>
<pre><code> https://developer.nvidia.com/cuda-downloads</code></pre>
<p>控制台输入以下命令：</p>
<pre><code> sudo dpkg -i cuda-repo-&lt;distro&gt;_&lt;version&gt;_&lt;architecture&gt;.deb
        sudo apt-get update
        sudo apt-get install cuda</code></pre>
<p>5)配置环境变量和lib库路径 安装完成后需要在/etc/profile中添加环境变量, 在文件最后添加:</p>
<pre><code>PATH=/usr/local/cuda-7.0/bin:$PATH
export PATH
    </code></pre>
<p>保存后使得环境变量立刻生效(或者重新打开控制台)</p>
<pre><code>source /etc/profile
    </code></pre>
<p>在 /etc/ld.so.conf.d/加入文件 cuda.conf, 内容如下</p>
<pre><code>/usr/local/cuda-7.0/lib64</code></pre>
<p>6）验证是否安装正确:</p>
<pre><code>cd /usr/local/cuda-7.0/bin/
cuda-install-samples-7.0.sh &lt;dir&gt; 
    </code></pre>
<p>dir为目标路径，脚本会把sample文件复制到指定路径下,之后进入该路径make就好了</p>
<p>编译完成后进入bin/x86_64/linux/release</p>
<p>执行./deviceQuery 如显示GPU信息说明cuda安装正确</p>
<h3 id="安装atlas">3 安装atlas</h3>
<p>使用gpu加速需要安装blas库，这里选择atlas</p>
<pre><code>sudo apt-get install libatlas-base-dev</code></pre>
<h3 id="下载caffe安装包与安装环境">4 下载caffe安装包与安装环境</h3>
<p>控制台输入:</p>
<pre><code>git clone https://github.com/BVLC/caffe</code></pre>
<p>也可以到github上下载zip文件解压</p>
<p>安装caffe的依赖库:</p>
<pre><code>sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev
sudo apt-get install --no-install-recommends libboost-all-dev
sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler</code></pre>
<h3 id="安装anaconda">5 安装anaconda</h3>
<p>不使用ubuntu自带的python是因为anaconda集成了许多科学计算库如numpy、scipy等，而且安装theano也很方便 在anaconda官网上下载安装包直接安装即可，默认安装路径为</p>
<pre><code>~/anaconda</code></pre>
<p>添加bin到PATH环境变量即可 打开控制台输入python确认使用的是anaconda里面的python 安装pip</p>
<pre><code>conda install pip</code></pre>
<h3 id="安装caffe所需要的python环境">6 安装Caffe所需要的Python环境</h3>
<p>然后执行如下命令安装编译caffe python wrapper 所需要的额外包</p>
<pre><code>cd python
for req in $(cat requirements.txt); do sudo pip install $req; done</code></pre>
<p>在运行Caffe时，可能会报一些找不到libxxx.so的错误，而用 locate libxxx.so命令发现已经安装在anaconda中，这时首先想到的是在/etc/ld.so.conf.d/ 下面将 $your_anaconda_path/lib 加入 LD_LIBRARY_PATH中。 但是这样做可能导致登出后无法再进入桌面！！！原因（猜测）可能是anaconda的lib中有些内容于系统自带的lib产生冲突。</p>
<p>正确的做法是：为了不让系统在启动时就将anaconda/lib加入系统库目录，可以在用户自己的~/.bashrc 中添加library path，比如我就在最后添加了两行</p>
<pre><code># add library path
LD_LIBRARY_PATH=your_anaconda_path/lib:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH</code></pre>
<p>开启另一个终端后即生效，并且重启后能够顺利加载lightdm, 进入桌面环境。</p>
<p>但在实际安装时注意先要等make和make test完之后再加这个路径否则会报错</p>
<h3 id="编译caffe">7 编译caffe</h3>
<p>完成了所有环境的配置，可以愉快的编译Caffe了！ 进入caffe根目录， 首先复制一份Makefile.config</p>
<pre><code>cp Makefile.config.example Makefile.config</code></pre>
<p>然后修改里面的内容，主要需要修改的参数包括</p>
<p>BLAS (默认使用的是atlas)</p>
<p>DEBUG 是否使用debug模式，打开此选项则可以在eclipse或者NSight中debug程序</p>
<p>如要安装python caffe和mat caffe的话需要在Makefile.config中指定python路径和matlab路径</p>
<p>完成设置后， 开始编译</p>
<pre><code>make all
make test
make runtest</code></pre>
<p>前两步成功说明caffe编译成功，make runtest是在运行caffe的各个测试脚本</p>
<h3 id="编译pycaffe">8 编译pycaffe</h3>
<p>控制台输入: make pycaffe</p>
<p>之后设置环境变量 在～/.bashrc中添加</p>
<pre><code>PYTHONPATH=/path/tp/caffe/python:$PYTHONPATH
export PYTHONPATH</code></pre>
<p>打开控制台输入python，之后import caffe，import成功就说明caffe安装成功了</p>
<h3 id="安装cudnn可选">9 安装cudnn（可选）</h3>
<h4 id="编译caffe之后安装cudnn">9.1编译caffe之后安装cudnn</h4>
<p>使用cudnn来加速GPU计算，cudnn支持caffe，theano和Torch7 在官网免费获得cudnn压缩包 下载之后控制台输入:</p>
<pre><code>tar -xzvf cudnn-6.5-linux-R1.tgz
cd cudnn-6.5-linux-R1
sudo cp lib* /usr/local/cuda/lib64/
sudo cp cudnn.h /usr/local/cuda/include/</code></pre>
<p>之后建立软链接（首先删除原先文件夹下的软链接）:</p>
<pre><code>cd /usr/local/cuda/lib64/
sudo rm -rf libcudnn.so libcudnn.so.6.5</code></pre>
<p>然后修改文件权限，并创建新的软连接:</p>
<pre><code>sudo chmod u=rwx,g=rx,o=rx libcudnn.so.6.5.18（未必是18，我的机子上是48，相应改就可以了） 
sudo ln -s libcudnn.so.6.5.18 libcudnn.so.6.5
sudo ln -s libcudnn.so.6.5 libcudnn.so</code></pre>
<h4 id="编译caffe之前安装cudnn">9.2编译caffe之前安装cudnn</h4>
<p>解压cudnn之后只需将对应的文件复制到对应目录中即可，同时在编译caffe时Makefile.config中</p>
<pre><code>use_cudnn:=1</code></pre>
<p>去掉注释</p>
<pre><code>unpack the library  
gzip -d cudnn-6.5-linux-x64-v2.tar.gz  
tar xf cudnn-6.5-linux-x64-v2.tar  

copy the library files into CUDA&#39;s include and lib folders  
sudo cp cudnn-6.5-linux-x64-v2/cudnn.h /usr/local/  cuda-7.0/include  
sudo cp cudnn-6.5-linux-x64-v2/libcudnn* /usr/local/    cuda-7.0/lib64      </code></pre>
<h3 id="安装opencv可选">10 安装opencv(可选)</h3>
<p>opencv库在运行其他开源项目时可能需要用到因此也建议安装，这里安装的是支持GPU的，因此安装是需要添加支持GPU的选项</p>
<p>opencv安装起来神烦，Github上有人已经写好了完整的安装脚本： https://github.com/jayrambhia/Install-OpenCV</p>
<p>下载该脚本，进入Ubuntu/2.4 目录, 给所有shell脚本加上可执行权限： chmod +x *.sh</p>
<p>安装2.4.9版本: sudo ./opencv2_4_9.sh</p>
<p>该脚本会去尝试下载2.4.9的压缩包，如果下载速度太慢建议还是自己先下载好，再把脚本中下载的语句注释掉，sh文件需要修改的是在cmake的那一行中添加一个编译参数 -D BUILD_TIFF=ON，否则在make caffe的时候会报类似</p>
<pre><code>/usr/lib/libopencv_highgui.so.2.4: undefined reference to TIFFRGBAImageOK@LIBTIFF_4.0&#39; 1&gt; </code></pre>
<p>的错误</p>
<p>安装过程中可能会遇到两个问题:</p>
<ul>
<li><p>1编译过程中报错：</p>
<pre><code>opencv-2.4.9/modules/gpu/src/nvidia/core/NCVPixelOperations.hpp(51): error: a storage class is not allowed in an explicit specialization</code></pre>
<p>http://www.samontab.com/web/2014/06/installing-opencv-2-4-9-in-ubuntu-14-04-lts/</p></li>
</ul>
<p>解决方法在此：http://code.opencv.org/issues/3814 下载 NCVPixelOperations.hpp 替换掉opencv2.4.9内的文件， 重新build</p>
<ul>
<li><p>2编译过程中报错：</p>
<pre><code>nvcc fatal: Unsupported gpu architecture: &#39;compute xx&#39;</code></pre>
<p>需要在cmake时添加参数指定你的GPU架构，参考这两篇blog:</p>
<p>http://blog.csdn.net/sysuwuhongpeng/article/details/45485719 http://blog.csdn.net/altenli/article/details/44199539</p></li>
</ul>
<h3 id="caffe-matlab可选">11 caffe matlab(可选)</h3>
<h4 id="安装matlab">1 安装matlab：</h4>
<p>参考教程：</p>
<p>Caffe提供了MATLAB接口， 有需要用MATLAB的同学可以额外安装MATLAB。安装教程请自行搜索。</p>
<p>安装完成后添加图标 matlab图标在该网站可以下载</p>
<pre><code>http://www.linuxidc.com/Linux/2011-01/31632.htm</code></pre>
<p>图标放到/usr/local/MATLAB/下</p>
<p>控制台输入以下内容</p>
<pre><code>sudo vi /usr/share/applications/Matlab.desktop</code></pre>
<p>复制代码</p>
<pre><code>[Desktop Entry]
Type=Application
Name=Matlab
GenericName=Matlab R2014a
Comment=Matlab:The Language of Technical Computing
Exec=sh /usr/local/MATLAB/R2014a/bin/matlab -desktop
Icon=/usr/local/MATLAB/Matlab.png
Terminal=false
Categories=Development;Matlab;</code></pre>
<h4 id="matlab-wrapper">2 matlab wrapper</h4>
<p>控制台输入：</p>
<pre><code>make matcaffe</code></pre>
<p>安装完后在运行matlab demo时遇到报错</p>
<pre><code>“libhdf5.so.6 no such file or directory”</code></pre>
<p>但是到anaconda的lib目录下发现是有对应文件的，可能是动态链接库的问题，解决方法是到anaconda/lib目录下执行ldconfig就可以了</p>
<h2 id="安装theano">安装theano</h2>
<h3 id="安装theano-1">1 安装theano</h3>
<p>控制台输入 pip install theano即可</p>
<h3 id="配置theano使其支持gpu加速">2 配置theano使其支持gpu加速</h3>
<p>参考文档： http://deeplearning.net/software/theano/tutorial/using_gpu.html http://deeplearning.net/software/theano/install.html#gpu-linux http://deeplearning.net/software/theano/library/config.html#config.init_gpu_device</p>
<p>在用户根目录下新建.theanorc配置文档，配置如下：</p>
<pre><code>[global]
floatX = float32
device = gpu

[nvcc]
fastmath = True

[blas]
ldflags = -lf77blas -latlas -lgfortran #put your flags here</code></pre>
<p>检验是否theano可以使用gpu加速，测试脚本如下：</p>
<pre><code>from theano import function, config, shared, tensor, sandbox
import numpy
import time

vlen = 10 * 30 * 768  # 10 x #cores x # threads per core
iters = 1000

rng = numpy.random.RandomState(22)
x = shared(numpy.asarray(rng.rand(vlen), config.floatX))
f = function([], tensor.exp(x))
print f.maker.fgraph.toposort()
t0 = time.time()
for i in xrange(iters):
    r = f()
t1 = time.time()
print &#39;Looping %d times took&#39; % iters, t1 - t0, &#39;seconds&#39;
print &#39;Result is&#39;, r
if numpy.any([isinstance(x.op, tensor.Elemwise) and
              (&#39;Gpu&#39; not in type(x.op).__name__)
              for x in f.maker.fgraph.toposort()]):
    print &#39;Used the cpu&#39;
else:
    print &#39;Used the gpu&#39;
如果显示used the gpu说明theano在使用gpu加速</code></pre>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="John Yao" itemprop="image"/>
          <p class="site-author-name" itemprop="name">John Yao</p>
        </div>
        <p class="site-description motion-element" itemprop="description">Dubers Blog</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">6</span>
              <span class="site-state-item-name">文章</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">4</span>
              <span class="site-state-item-name">分類</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">5</span>
              <span class="site-state-item-name">標籤</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Yao</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 強力驅動
</div>

<div class="theme-info">
  主題 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"dubur"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
