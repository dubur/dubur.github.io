<!doctype html>
<html class="theme-next use-motion theme-next-mist">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>




<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>




  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Dubur">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Dubur">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dubur">
<meta name="twitter:description">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: 'Mist',
    sidebar: 'post'
  };
</script>

  <title> Dubur </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  

  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?de04554947ef5ddb1d9dd95ef6598c19";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">Dubur</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            <i class="menu-item-icon icon-next-about"></i> <br />
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            标签
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/16/Faster RCNN论文笔记/" itemprop="url">
                Faster RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-08-16T17:10:30+08:00" content="2015-08-16">
            2015-08-16
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/16/Faster RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/16/Faster RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>本笔记中会多次提到Fast RCNN的架构，对Fast RCNN的概念与架构有疑问的可以参考</p>
<p><a href="http://arxiv.org/pdf/1504.08083v1.pdf" target="_blank" rel="external">Fast RCNN</a></p>
<p>或者我的Fast RCNN笔记</p>
<p><a href="http://dubur.github.io/2015/08/12/Fast%20RCNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" target="_blank" rel="external">Notes on Fast RCNN</a></p>
<p>MSR的研究员似乎对Fast RCNN的成果还不够满意，从RCNN到Fast RCNN，所有的detection任务都使用了selective search来提取region proposal，因此诞生了用神经网络来提取region proposal的方法，进一步提升检测速度的同时还提升了检测的准确率，然而代码现在还没有公开。</p>
<h2 id="Motivation">Motivation</h2><ul>
<li>Fast RCNN ignores the time spent on region proposals</li>
<li>Region proposal methods used in research are implemented on the CPU</li>
<li><strong><em>The feature maps can also be used for generating region proposals</em></strong><br>简单说就是提取proposal的步骤太花时间，而且作者希望尽可能用GPU完成所有的计算。</li>
</ul>
<h2 id="Region_Proposal_Networks_(RPN)">Region Proposal Networks (RPN)</h2><h3 id="Basic_Notions">Basic Notions</h3><ul>
<li>Fully-convolutional network for generating region proposals</li>
<li>Share computation of convolutions with start-of-art object detection networks</li>
</ul>
<h3 id="Architecture">Architecture</h3><ul>
<li>RPN与卷积层共享权重，也就是说RPN的输入就是最后一个卷积层的输出的feature map，得到输入后使用滑动窗口的方式得到更低维的向量</li>
<li><p>将得到的向量输入到两个并联的全连接层</p>
<ol>
<li>box-regression layer (bounding box regressor)</li>
<li>box-classification layer (objectness)<br>类似于Fast RCNN的RoI pooling层后面的架构，feature用来训练两个分类器，一个用来判断这个RoI是否包含object，另一个用来做bounding box回归(即给定一个anchor判断bounding box的位置)</li>
</ol>
</li>
<li><p>每个滑动窗口中心记作一个anchor，对应9种variant，相当于对于每一个feature map用9种sliding window计算vector，后面的输出也变成9倍，这么做是为了对图像的translation有更好的鲁棒性</p>
<p><img src="img/RPN.png" alt="RPN"></p>
</li>
<li><p>Loss Function的定义与Fast RCNN类似</p>
</li>
</ul>
<p>$$ L(p_{i},t_{i}) = L_{cls}(p_{i},{p_{i}}^{*}) + \lambda{p_{i}}^{*}L_{reg}(t_{i},{t_{i}}^{*}) $$</p>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>

<p>  由两部分构成：regression的Loss和classification的Loss，通过$\lambda$控制两类Loss的比重<br>  公式中$p_{i}$表示一个region中包含object的概率，${p_{i}}^{*}$表示ground truth， 是一个0-1指示器，当某个anchor被标注为含有一个object时它的值才是1，否则为0, $L_{cls}(p_{i},{p_{i}}^{*})$的定义方式与Fast RCNN中相同</p>
<h2 id="Training_RPN">Training RPN</h2><ul>
<li>mini batch构成<ol>
<li>随机选取一张图片的256个anchor，正负样本为1:1</li>
<li>用高斯分布初始化权重</li>
</ol>
</li>
<li><p>Trainging的四个步骤<br>这里引用一下原文:</p>
<ol>
<li>First, train the RPN is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task. </li>
<li>In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection network is also initialized by the ImageNet-pre-trained model. </li>
<li>In the third step, we use the detector network to initialize RPN training, but we fix the shared conv layers and only fine-tune the layers unique to RPN. </li>
<li>Now the two networks share conv layers. Finally, keeping the shared conv layers fixed, we fine-tune the fc layers of the Fast R-CNN.</li>
</ol>
<p>总体而言训练方式在训练Fast RCNN的基础上做了改进，Fast RCNN输入的region proposal由RPN提供，之后用Fast RCNN的权重重新初始化RPN的参数，做到权重共享，保持这些权重不变，只对RPN中的几层进行微调，最后再对全连接层进行微调。</p>
<p>这样做的好处显而易见，实时检测时，前面卷积层提取feature map直接送给RPN， Fast RCNN等待RPN计算proposal，用GPU计算proposal速度很快，与Fast RCNN不同的是不需要selective search，提取proposal的步骤与detection合并，在原先的卷积层与RoI pooling层之间加入了一个RPN提取proposal，之后Fast RCNN的全连接层利用RPN的输出向量进行detection</p>
<p>笔者认为从RCNN到这里的Faster RCNN，有一点bounding box regression和object classification的joint learning的味道了。</p>
</li>
</ul>
<p>最后RPN结合Fast RCNN可以做到单纯用神经网络进行object detection，不依靠selective search这样的low-level feature的方法，完全依靠deep learning进行图像的理解。</p>
<p>不过有时间还是要把selective search也看一下，毕竟这篇论文有很高的引用量。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/12/RCNN论文笔记/" itemprop="url">
                RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-08-12T16:17:30+08:00" content="2015-08-12">
            2015-08-12
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/12/RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/12/RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>RCNN本身概念比较简单，但是论文中提到了许多object detection的经典方法非常有用，这些内容绝大部分在附录中，在阅读其他论文时相信会有很大帮助。</p>
<h2 id="Basic_Notions">Basic Notions</h2><ul>
<li>Pipeline method<ol>
<li>提取独立于物体类别的region proposal</li>
<li>使用CNN做特征提取</li>
<li>调整CNN参数用于从候选的proposal中获取包含object的proposal(domain-specific fine-tuning)</li>
<li>SVM进行分类</li>
</ol>
</li>
</ul>
<h2 id="RCNN_for_Object_Detection">RCNN for Object Detection</h2><h3 id="Testing">Testing</h3><ul>
<li>Selective search 提取region proposal(非本文重点)</li>
<li>使用已经训练好的CNN(dataset: ILSVRC2012 classification caffe implementation) 提取proposal的特征，为了将输入标准化，强制将每个proposal resize到227*227 </li>
<li>CNN提取的特征输入到SVM中进行分类</li>
</ul>
<h3 id="Training">Training</h3><p>按照个人理解，为了让CNN有detection的能力，我们需要让网络具备分辨哪些proposal可能包含object，因为selective search选取的许多proposal其实没有object或者仅包含了object的一小部分。</p>
<p>因此文中加了一步对CNN参数的调整(domain-specific fine-tuning),主要目的就是筛选proposal</p>
<p>在实验部分作者也进行了对比，结果证明使用了fine-tuning比不使用要高8%的mAP</p>
<ul>
<li><p>Supervised pre-training</p>
<ol>
<li>CNN on ILSVRC2012 with 2.2% error rate</li>
<li>training set: images with image-level annotations, <strong><em>no bounding boxes</em></strong></li>
</ol>
</li>
<li><p>Domain specific fine-tuning</p>
<ol>
<li>N+1 classification: N object classes and 1 background</li>
<li>Use warped region proposals for tuning</li>
<li>Positive training examples: region proposals with &gt;0.5IoU overlap(the rest as negative)</li>
</ol>
</li>
<li><p>Object category classification</p>
<ol>
<li>Positive examples: ground-truth boxes, Negative: region proposals with &lt;0.3IoU overlap</li>
<li>Trade-off between SVM and Softmax</li>
<li>Different definition for training sets between fine-tuning and classification</li>
</ol>
</li>
</ul>
<p>这里作者提了两个问题 (在附录里均有解答):</p>
<ol>
<li>为什么在第二步调整CNN参数使用的训练集和训练SVM分类器的训练集不同(尤其是threshold的选择不一样)?</li>
<li>为什么不使用softmax分类器而选择SVM?  </li>
</ol>
<h2 id="Appendix">Appendix</h2><ul>
<li><p>Object proposal transformations<br>由于CNN输入是227*227的正方形图像，因此需要对proposal进行缩放处理，文中提出了3种不同的处理方式：</p>
<ol>
<li>tightest square with context(等比例缩放proposal在原始图像中的bounding square)</li>
<li>tightest square without context(等比例缩放proposal在原始图像中的bounding square,但要去掉在bounding square中而不在原始proposal中的部分)</li>
<li>强制缩放proposal<br>3种变形的方式如图中所示:</li>
</ol>
<p><img src="/img/transformation.jpeg" alt="transformation"></p>
<p>(A) origin proposal (B) tightest square with context (C) tightest square without context (D) warp</p>
</li>
<li><p>Positive vs negative examples and softmax</p>
<ol>
<li>用于fine－tuning的数据集过少，因此将降低了positive example的要求，只要IoU大于0.5就认为可能有object在这个proposal里面，作者同时指出这样能够防止overfitting</li>
<li>使用softmax比SVM降低3.3%的mAP，主要原因可能在于fine-tuning的训练集不那么强调box的位置，且SVM使用的nagative sample更具有区分度</li>
</ol>
</li>
<li>Bounding-box 回归<ol>
<li>使用最后一个pool层的输出训练线性回归模型来预测detection window</li>
<li>实验结果表明使用BB regression效果更优</li>
</ol>
</li>
</ul>
<h2 id="Insights">Insights</h2><ul>
<li>Most feature information is extracted by the convolutional layers</li>
<li>The structure of CNN really matters(nearly 7~8% mAP)</li>
</ul>
<p>下面两点是这篇文章的核心：</p>
<ul>
<li>High-capacity CNN to bottom-up region proposals in order to localize and segment objects(CNN用于detection)</li>
<li>Effective method: supervised pretraining (auxiliary data eg. classification)/fine-tuning(scarce data eg. detection)(fine-tuning的重要性不言而喻)</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/12/Fast RCNN论文笔记/" itemprop="url">
                Fast RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-08-12T16:17:30+08:00" content="2015-08-12">
            2015-08-12
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/12/Fast RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/12/Fast RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>Fast RCNN在RCNN的基础上修改了网络结构，同时加快了训练速度并且降低了训练成本，从工程角度说RCNN提出了思路而Fast RCNN的实用性更强</p>
<h2 id="Motivation_(Drawbacks_of_RCNN)">Motivation (Drawbacks of RCNN)</h2><ul>
<li>RCNN的训练方式按照一个一个阶段分的比较开，</li>
<li>训练时RCNN会将每个proposal的特征向量存储在硬盘上，不仅占用存储空间，时间上也因为读写磁盘而变得很慢</li>
<li>在进行detection测试时耗时长</li>
</ul>
<h2 id="Fast_RCNN_Training">Fast RCNN Training</h2><p>文章的亮点在于对原来网络结构的改进，这里就不得不膜拜大神了</p>
<ul>
<li><p>RoI Pooling Layer<br>按照作者所述，该层的思想类似于SPPnet，我们假设最后一个卷积层得到输入N个feature map(这些feature map包含原始图像的所有信息)，而我们只想提取感兴趣的部分，因此这就需要引入RoI了</p>
<p>将这N个feature map和R个region proposal输入到RoI pooling层中，根据RoI中指定的区域对原始的feature map进行pooling得到这R个region proposal的N个feature map</p>
<p>笔者觉得其实和SPPnet没有多大的关系……由于各个RoI的大小不一样，而为了使得每个RoI pooling出来的feature长度相同，采用不同大小的pooling窗口而已</p>
</li>
<li><p>对原始CNN网络结构的修改<br>修改主要有3处</p>
<ol>
<li>将最后一个max pooling层替换为RoI pooling层</li>
<li>将最后一个全连接层和softmax层替换为两个同并列的层<br>(1) K+1个class的分类器<br>(2) Bounding box回归(Localization) </li>
<li>修改网络的输入使其接收N个image和R个RoI<br>最后网络结构如图所示：<img src="/img/fast-rcnn.jpeg" alt="fast-rcnn"></li>
</ol>
<p>核心在于使用RoI pooling层直接提取原始feature map中属于RoI的那一部分</p>
</li>
</ul>
<ul>
<li><p>Fine-tuning微调</p>
<p>  网络结构变化带来的是训练方式的变化，作者在现有的CNN分类网络上进行了微调，类似于RCNN中的domain-specific fine-tuning,只不过这次是针对网络结构而调整训练方式</p>
<ol>
<li><p>Multi-task Loss</p>
<p>简单来说就是将分类的Loss和bounding box回归的Loss做了加权和</p>
<p>$$ L(p,k^*,t,t^*) = L_{cls}(p,k^*) + \lambda[k^*\ &gt;\ 1]L_{loc}(t,t^*) $$</p>
<p>$$ smooth_{L_1}(x) = \begin{cases}0.5x^2 \ &amp;if \ |x| &lt; 1<br>\cr |x|-0.5 \ &amp;otherwise \end{cases} $$</p>
<p>第一部分是softmax分类器的Loss，第二部分是regression的Loss，值得注意的是第二部分有个指示函数，说明不会计算那些background类型的proposal的locaolization误差</p>
</li>
<li><p>mini-batch sampling</p>
<p>每一次SGD的batch使用2个image和128个RoI，每个batch中RoI的构成:<br>1.25% ground truth中的bounding box IoU&gt;0.5,<br>2.75% IoU &lt; 0.5, 看作background example</p>
</li>
<li><p>BP through RoI pooling layer</p>
<p>在进行反向传播时RoI pooling layer有些许不同, 参与pooling的不再是整幅图像而是可能有重叠的RoI区域，计算公式如下:</p>
<p>$$ {\partial L \over \partial x} = \sum_{r\in R} \sum_{y\in r} [y\ pooled\ x] {\partial L \over \partial y} $$</p>
<p>y代表pooling后的值, 反向传播的思想是要将每个y的偏导传递给所有参与pooling的变量x，这也就是指示函数 [y pooled x] 的含义，在这里x代表的是原来RoI中的元素</p>
</li>
</ol>
</li>
<li><p>Scale invariance<br>如何处理输入图像尺寸不一的问题？<br>作者给出了两种方案:</p>
<ul>
<li>brute force 强制把图像resize到同一尺寸下</li>
<li>image pyramids 利用图像金字塔，把图片resize成不同的大小，选取其中最接近optimal scale（也就是227*227）的大小用作训练。</li>
</ul>
</li>
</ul>
<h2 id="Fast_RCNN_Detection">Fast RCNN Detection</h2><p><script type="text/x-mathjax-config"><br>  MathJax.Hub.Config({tex2jax: {inlineMath: [[‘$’,’$’], [‘\(‘,’\)’]]}});<br></script></p>
<ul>
<li><p>Truncated SVD<br>全连接层的计算时间太长，占用了整个前向传播的一半时间，因此在进行矩阵乘法时，作者采用SVD将权重矩阵$W (u \times v)$分解为三个矩阵的乘积：</p>
<p>$$ W \approx U \Sigma_tV^T $$</p>
<p>SVD分解后的U和V都是对称矩阵<br>最后一个全连接层被分解为两个子层:第一层的权重矩阵为$ \Sigma_tV^T $,第二层的权重矩阵为$U$,因此整个参数的个数由$uv$变成了$t(u+v)$,当t比较小(意味着原参数矩阵的特征值少)的时候计算时间就会大大减少</p>
<p><img src="img/training_time.jpeg" alt="training_time"></p>
</li>
</ul>
<h2 id="Experiment_Insights">Experiment Insights</h2><ul>
<li>卷积层和全连接层一样需要fine-tuning<blockquote>
<p>decrease mAP from 66.9% to 61.4%</p>
</blockquote>
</li>
</ul>
<p>，且浅层的卷积层没有微调的必要。</p>
<ul>
<li>首先用classification loss训练，之后加入bbox regression训练效果更好<blockquote>
<p>improvement ranges from +0.8 to +1.1 mAP points</p>
</blockquote>
</li>
</ul>
<p>(那为什么前面在写网络结构时还给出sibling layer的训练方式…)</p>
<ul>
<li>trade-off: speed &gt; performance improvement brought by multi-scale </li>
<li>More data brings higher mAP</li>
<li>Softmax slightly outperforms SVM </li>
<li>Sparse proposals(selective search) improve detection quality and dense proposals(sliding window) free the heavy running cost.</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/06/theano-and-caffe-installation/" itemprop="url">
                Ubuntu14.04 安装 theano 和 caffe
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          发表于
          <time itemprop="dateCreated" datetime="2015-08-06T23:34:01+08:00" content="2015-08-06">
            2015-08-06
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分类于
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Instructions/" itemprop="url" rel="index">
                  <span itemprop="name">Instructions</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/06/theano-and-caffe-installation/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/06/theano-and-caffe-installation/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>说明:安装时为了支持python版本，且为了安装方便，最好首先安装anaconda，anaconda安装起来方便并且集成了许多科学计算库，主要为了配合theano使用</p>
<p>如果想安装opencv、添加caffe matlab等等最好先看教程中相关部分最后再编译caffe比较保险</p>
<h3 id="安装caffe">安装caffe</h3><h4 id="1-安装build-essentials">1.安装build-essentials</h4><p>安装开发所需要的基本包（一般装完系统就有）<br>sudo apt-get install build-essential</p>
<h4 id="2-安装NVIDIA驱动">2.安装NVIDIA驱动</h4><h5 id="2-1_退出图形界面">2.1 退出图形界面</h5><pre><code><span class="number">1</span>）由于有的带有gpu的电脑在启动时默认使用独显作为主要显示设备，因此我们需要将<span class="keyword">bios设置改为使用集显作为显示设备
</span><span class="number">2</span>）进入ubuntu，按ctrl＋alt＋<span class="literal">F1</span>进入tty，登陆tty后输入：
    sudo service lightdm stop
其他desktop manager也需要关闭
</code></pre><h5 id="2-2安装驱动">2.2安装驱动</h5><p>准备工作：</p>
<p>1）Verify the system has a CUDA-capable GPU.</p>
<p>控制台输入以下命令：</p>
<pre><code>lspci <span class="string">| grep -i nvidia</span>
</code></pre><p>如果列出了当前NIVDIA显卡的信息则说明电脑的GPU事CUDA-capable的</p>
<p>2）Verify the system is running a supported version of Linux.</p>
<p>控制台输入以下命令:</p>
<pre><code>uname -<span class="keyword">m</span> &amp;&amp; <span class="keyword">cat</span> /etc<span class="comment">/*release</span>
</code></pre><p>输出的结果中如果显示x86_64则说明电脑是x86架构的，在下载驱动安装包时选择对应的包即可</p>
<p>3）Verify the system has gcc installed.</p>
<p>控制台输入:</p>
<pre><code>gcc <span class="comment">--version</span>
</code></pre><p>确认gcc版本正确</p>
<p>4）Download the NVIDIA CUDA Toolkit.</p>
<p>从NVIDIA官网上下载对应的驱动安装包<br>下载地址:</p>
<pre><code><span class="string">https:</span><span class="comment">//developer.nvidia.com/cuda-downloads</span>
</code></pre><p>控制台输入以下命令：</p>
<pre><code>sudo dpkg -<span class="keyword">i</span> cuda-repo-&lt;distro&gt;_&lt;<span class="keyword">version</span>&gt;_&lt;architecture&gt;.<span class="keyword">deb</span>
        sudo apt-<span class="built_in">get</span> <span class="keyword">update</span>
        sudo apt-<span class="built_in">get</span> install cuda
</code></pre><p>5)配置环境变量和lib库路径<br>安装完成后需要在/etc/profile中添加环境变量, 在文件最后添加:</p>
<pre><code>PATH=/usr/local/cuda-<span class="number">7.0</span>/bin:<span class="variable">$PATH</span>
export PATH
</code></pre><p>保存后使得环境变量立刻生效(或者重新打开控制台)</p>
<pre><code><span class="keyword">source</span> <span class="regexp">/etc/</span>profile
</code></pre><p>在 /etc/ld.so.conf.d/加入文件 cuda.conf, 内容如下</p>
<pre><code><span class="regexp">/usr/</span>local<span class="regexp">/cuda-7.0/</span>lib64
</code></pre><p>6）验证是否安装正确:</p>
<pre><code><span class="keyword">cd</span> /usr/<span class="keyword">local</span>/cuda-7.0/bin/
cuda-install-samples-7.0.<span class="keyword">sh</span> &lt;<span class="keyword">dir</span>&gt; 
</code></pre><p>dir为目标路径，脚本会把sample文件复制到指定路径下,之后进入该路径make就好了</p>
<p>编译完成后进入bin/x86_64/linux/release</p>
<p>执行./deviceQuery 如显示GPU信息说明cuda安装正确</p>
<h4 id="3_安装atlas">3 安装atlas</h4><p>使用gpu加速需要安装blas库，这里选择atlas</p>
<pre><code>sudo apt-<span class="keyword">get</span> install libatlas-<span class="keyword">base</span>-dev
</code></pre><h4 id="4_下载caffe安装包与安装环境">4 下载caffe安装包与安装环境</h4><p>控制台输入: </p>
<pre><code>git clone <span class="string">https:</span><span class="comment">//github.com/BVLC/caffe</span>
</code></pre><p>也可以到github上下载zip文件解压</p>
<p>安装caffe的依赖库:</p>
<pre><code>sudo apt-<span class="built_in">get</span> install libprotobuf-<span class="built_in">dev</span> libleveldb-<span class="built_in">dev</span> libsnappy-<span class="built_in">dev</span> libopencv-<span class="built_in">dev</span> libhdf5-serial-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> install --no-install-recommends libboost-<span class="built_in">all</span>-<span class="built_in">dev</span>
sudo apt-<span class="built_in">get</span> install libgflags-<span class="built_in">dev</span> libgoogle-glog-<span class="built_in">dev</span> liblmdb-<span class="built_in">dev</span> protobuf-compiler
</code></pre><h4 id="5_安装anaconda">5 安装anaconda</h4><p>不使用ubuntu自带的python是因为anaconda集成了许多科学计算库如numpy、scipy等，而且安装theano也很方便<br>    在anaconda官网上下载安装包直接安装即可，默认安装路径为</p>
<pre><code>~/anaconda
</code></pre><p>添加bin到PATH环境变量即可<br>打开控制台输入python确认使用的是anaconda里面的python<br>安装pip</p>
<pre><code>conda <span class="keyword">install</span> pip
</code></pre><h4 id="6_安装Caffe所需要的Python环境">6 安装Caffe所需要的Python环境</h4><p>然后执行如下命令安装编译caffe python wrapper 所需要的额外包</p>
<pre><code>cd python
<span class="keyword">for</span> req <span class="keyword">in</span> $(cat requirements.txt); do sudo pip install <span class="variable">$req</span>; done
</code></pre><p>在运行Caffe时，可能会报一些找不到libxxx.so的错误，而用 locate libxxx.so命令发现已经安装在anaconda中，这时首先想到的是在/etc/ld.so.conf.d/ 下面将 $your_anaconda_path/lib 加入 LD_LIBRARY_PATH中。 但是这样做可能导致登出后无法再进入桌面！！！原因（猜测）可能是anaconda的lib中有些内容于系统自带的lib产生冲突。</p>
<p>正确的做法是：为了不让系统在启动时就将anaconda/lib加入系统库目录，可以在用户自己的~/.bashrc 中添加library path，比如我就在最后添加了两行</p>
<pre><code><span class="comment"># add library path</span>
LD_LIBRARY_PATH=your_anaconda_path/lib:<span class="variable">$LD_LIBRARY_PATH</span>
<span class="built_in">export</span> LD_LIBRARY_PATH
</code></pre><p>开启另一个终端后即生效，并且重启后能够顺利加载lightdm, 进入桌面环境。</p>
<p>但在实际安装时注意先要等make和make test完之后再加这个路径否则会报错</p>
<h4 id="7_编译caffe">7 编译caffe</h4><p>完成了所有环境的配置，可以愉快的编译Caffe了！ 进入caffe根目录， 首先复制一份Makefile.config</p>
<pre><code><span class="tag">cp</span> <span class="tag">Makefile</span><span class="class">.config</span><span class="class">.example</span> <span class="tag">Makefile</span><span class="class">.config</span>
</code></pre><p>然后修改里面的内容，主要需要修改的参数包括</p>
<p>BLAS (默认使用的是atlas)</p>
<p>DEBUG 是否使用debug模式，打开此选项则可以在eclipse或者NSight中debug程序</p>
<p>如要安装python caffe和mat caffe的话需要在Makefile.config中指定python路径和matlab路径</p>
<p>完成设置后， 开始编译</p>
<pre><code><span class="keyword">make</span> <span class="keyword">all</span>
<span class="keyword">make</span> test
<span class="keyword">make</span> runtest
</code></pre><p>前两步成功说明caffe编译成功，make runtest是在运行caffe的各个测试脚本</p>
<h4 id="8_编译pycaffe">8 编译pycaffe</h4><p>控制台输入:<br>    make pycaffe </p>
<p>之后设置环境变量<br>在～/.bashrc中添加</p>
<pre><code>PYTHONPATH=/path/tp/caffe/python:<span class="variable">$PYTHONPATH</span>
export PYTHONPATH
</code></pre><p>打开控制台输入python，之后import caffe，import成功就说明caffe安装成功了</p>
<h4 id="9_安装cudnn（可选）">9 安装cudnn（可选）</h4><h5 id="9-1编译caffe之后安装cudnn">9.1编译caffe之后安装cudnn</h5><p>使用cudnn来加速GPU计算，cudnn支持caffe，theano和Torch7<br>在官网免费获得cudnn压缩包<br>下载之后控制台输入:</p>
<pre><code><span class="label">tar</span> -xzvf cudnn-<span class="number">6</span>.<span class="number">5</span>-linux-<span class="literal">R1</span>.tgz
<span class="label">cd</span> cudnn-<span class="number">6</span>.<span class="number">5</span>-linux-<span class="literal">R1</span>
<span class="label">sudo</span> <span class="preprocessor">cp</span> lib* /usr/local/cuda/lib64/
<span class="label">sudo</span> <span class="preprocessor">cp</span> cudnn.h /usr/local/cuda/<span class="preprocessor">include</span>/
</code></pre><p>之后建立软链接（首先删除原先文件夹下的软链接）:</p>
<pre><code><span class="keyword">cd</span> /usr/<span class="keyword">local</span>/cuda/lib64/
sudo <span class="keyword">rm</span> -rf libcudnn.<span class="keyword">so</span> libcudnn.<span class="keyword">so</span>.6.5
</code></pre><p>然后修改文件权限，并创建新的软连接:</p>
<pre><code>sudo chmod u=rwx,g=rx,o=rx libcudnn.so<span class="number">.6</span><span class="number">.5</span><span class="number">.18</span>（未必是<span class="number">18</span>，我的机子上是<span class="number">48</span>，相应改就可以了） 
sudo ln -s libcudnn.so<span class="number">.6</span><span class="number">.5</span><span class="number">.18</span> libcudnn.so<span class="number">.6</span><span class="number">.5</span>
sudo ln -s libcudnn.so<span class="number">.6</span><span class="number">.5</span> libcudnn.so
</code></pre><h5 id="9-2编译caffe之前安装cudnn">9.2编译caffe之前安装cudnn</h5><p>解压cudnn之后只需将对应的文件复制到对应目录中即可，同时在编译caffe时Makefile.config中</p>
<pre><code><span class="label">use_cudnn:</span>=<span class="number">1</span>
</code></pre><p>去掉注释</p>
<pre><code>unpack the library  
gzip -<span class="keyword">d</span> cudnn-6.5-linux-x64-v2.tar.gz  
tar xf cudnn-6.5-linux-x64-v2.tar  

<span class="keyword">copy</span> the library files into CUDA's <span class="keyword">include</span> and lib folders  
sudo cp cudnn-6.5-linux-x64-v2/cudnn.<span class="keyword">h</span> /usr/<span class="keyword">local</span>/    cuda-7.0/<span class="keyword">include</span>  
sudo cp cudnn-6.5-linux-x64-v2/libcudnn* /usr/<span class="keyword">local</span>/    cuda-7.0/lib64      
</code></pre><h4 id="10_安装opencv(可选)">10 安装opencv(可选)</h4><p>opencv库在运行其他开源项目时可能需要用到因此也建议安装，这里安装的是支持GPU的，因此安装是需要添加支持GPU的选项</p>
<p>opencv安装起来神烦，Github上有人已经写好了完整的安装脚本：    <a href="https://github.com/jayrambhia/Install-OpenCV" target="_blank" rel="external">https://github.com/jayrambhia/Install-OpenCV</a></p>
<p>下载该脚本，进入Ubuntu/2.4 目录, 给所有shell脚本加上可执行权限：<br>    chmod +x *.sh</p>
<p>安装2.4.9版本:<br>    sudo ./opencv2_4_9.sh</p>
<p>该脚本会去尝试下载2.4.9的压缩包，如果下载速度太慢建议还是自己先下载好，再把脚本中下载的语句注释掉，sh文件需要修改的是在cmake的那一行中添加一个编译参数 -D BUILD_TIFF=ON，否则在make caffe的时候会报类似</p>
<pre><code>/usr/lib/libopencv_highgui.so.<span class="number">2.4</span>: <span class="literal">undefined</span> reference <span class="keyword">to</span> TIFFRGBAImageOK<span class="property">@LIBTIFF_4</span>.<span class="number">0</span><span class="string">' 1&gt; </span>
</code></pre><p>的错误</p>
<p>安装过程中可能会遇到两个问题:</p>
<ul>
<li><p>1编译过程中报错：</p>
<pre><code>opencv-<span class="number">2.4</span>.<span class="number">9</span>/modules/gpu/src/nvidia/core/NCVPixelOperations.<span class="function"><span class="title">hpp</span><span class="params">(<span class="number">51</span>)</span></span>: error: <span class="tag">a</span> storage class is not allowed <span class="keyword">in</span> an explicit specialization
</code></pre><p>  <a href="http://www.samontab.com/web/2014/06/installing-opencv-2-4-9-in-ubuntu-14-04-lts/" target="_blank" rel="external">http://www.samontab.com/web/2014/06/installing-opencv-2-4-9-in-ubuntu-14-04-lts/</a></p>
</li>
</ul>
<p>解决方法在此：<a href="http://code.opencv.org/issues/3814" target="_blank" rel="external">http://code.opencv.org/issues/3814</a>  下载 NCVPixelOperations.hpp 替换掉opencv2.4.9内的文件， 重新build</p>
<ul>
<li><p>2编译过程中报错：</p>
<pre><code>nvcc <span class="string">fatal:</span> Unsupported gpu <span class="string">architecture:</span> <span class="string">'compute xx'</span>
</code></pre><p>需要在cmake时添加参数指定你的GPU架构，参考这两篇blog:</p>
<p>  <a href="http://blog.csdn.net/sysuwuhongpeng/article/details/45485719" target="_blank" rel="external">http://blog.csdn.net/sysuwuhongpeng/article/details/45485719</a><br>  <a href="http://blog.csdn.net/altenli/article/details/44199539" target="_blank" rel="external">http://blog.csdn.net/altenli/article/details/44199539</a></p>
</li>
</ul>
<h4 id="11_caffe_matlab(可选)">11 caffe matlab(可选)</h4><h5 id="1_安装matlab：">1 安装matlab：</h5><p>参考教程：</p>
<p>Caffe提供了MATLAB接口， 有需要用MATLAB的同学可以额外安装MATLAB。安装教程请自行搜索。 </p>
<p>安装完成后添加图标 matlab图标在该网站可以下载</p>
<pre><code><span class="string">http:</span><span class="comment">//www.linuxidc.com/Linux/2011-01/31632.htm</span>
</code></pre><p>图标放到/usr/local/MATLAB/下</p>
<p>控制台输入以下内容</p>
<pre><code>sudo vi <span class="regexp">/usr/</span>share<span class="regexp">/applications/M</span>atlab.desktop
</code></pre><p>复制代码</p>
<pre><code><span class="title">[Desktop Entry]</span>
<span class="setting">Type=<span class="value">Application</span></span>
<span class="setting">Name=<span class="value">Matlab</span></span>
<span class="setting">GenericName=<span class="value">Matlab R2014a</span></span>
<span class="setting">Comment=<span class="value">Matlab:The Language of Technical Computing</span></span>
<span class="setting">Exec=<span class="value">sh /usr/local/MATLAB/R2014a/bin/matlab -desktop</span></span>
<span class="setting">Icon=<span class="value">/usr/local/MATLAB/Matlab.png</span></span>
<span class="setting">Terminal=<span class="value"><span class="keyword">false</span></span></span>
<span class="setting">Categories=<span class="value">Development;Matlab;</span></span>
</code></pre><h5 id="2_matlab_wrapper">2 matlab wrapper</h5><p>控制台输入：</p>
<pre><code><span class="built_in">make</span> matcaffe
</code></pre><p>安装完后在运行matlab demo时遇到报错</p>
<pre><code>“libhdf5.<span class="keyword">so</span>.<span class="number">6</span> <span class="keyword">no</span> such <span class="keyword">file</span> <span class="built_in">or</span> directory”
</code></pre><p>但是到anaconda的lib目录下发现是有对应文件的，可能是动态链接库的问题，解决方法是到anaconda/lib目录下执行ldconfig就可以了</p>
<h3 id="安装theano">安装theano</h3><h4 id="1_安装theano">1 安装theano</h4><p>控制台输入 pip install theano即可</p>
<h4 id="2_配置theano使其支持gpu加速">2 配置theano使其支持gpu加速</h4><p>参考文档：<br>    <a href="http://deeplearning.net/software/theano/tutorial/using_gpu.html" target="_blank" rel="external">http://deeplearning.net/software/theano/tutorial/using_gpu.html</a><br>    <a href="http://deeplearning.net/software/theano/install.html#gpu-linux" target="_blank" rel="external">http://deeplearning.net/software/theano/install.html#gpu-linux</a><br>    <a href="http://deeplearning.net/software/theano/library/config.html#config.init_gpu_device" target="_blank" rel="external">http://deeplearning.net/software/theano/library/config.html#config.init_gpu_device</a></p>
<p>在用户根目录下新建.theanorc配置文档，配置如下：</p>
<pre><code><span class="title">[global]</span>
<span class="setting">floatX = <span class="value">float32</span></span>
<span class="setting">device = <span class="value">gpu</span></span>
<span class="title">
[nvcc]</span>
<span class="setting">fastmath = <span class="value"><span class="keyword">True</span></span></span>
<span class="title">
[blas]</span>
<span class="setting">ldflags = <span class="value">-lf77blas -latlas -lgfortran #put your flags here</span></span>
</code></pre><p>检验是否theano可以使用gpu加速，测试脚本如下：</p>
<pre><code><span class="built_in">from</span> theano import <span class="function"><span class="keyword">function</span>, <span class="title">config</span>, <span class="title">shared</span>, <span class="title">tensor</span>, <span class="title">sandbox</span></span>
import numpy
import <span class="built_in">time</span>

vlen = <span class="number">10</span> * <span class="number">30</span> * <span class="number">768</span>  <span class="comment"># 10 x #cores x # threads per core</span>
iters = <span class="number">1000</span>

rng = numpy.<span class="built_in">random</span>.RandomState(<span class="number">22</span>)
x = shared(numpy.asarray(rng.rand(vlen), config.floatX))
f = <span class="function"><span class="keyword">function</span>([], <span class="title">tensor</span>.<span class="title">exp</span>(<span class="title">x</span>))</span>
print f.maker.fgraph.toposort()
t0 = <span class="built_in">time</span>.<span class="built_in">time</span>()
<span class="keyword">for</span> i <span class="operator">in</span> xrange(iters):
    r = f()
t1 = <span class="built_in">time</span>.<span class="built_in">time</span>()
print <span class="string">'Looping %d times took'</span> % iters, t1 - t0, <span class="string">'seconds'</span>
print <span class="string">'Result is'</span>, r
<span class="keyword">if</span> numpy.<span class="keyword">any</span>([isinstance(x.op, tensor.Elemwise) <span class="operator">and</span>
              (<span class="string">'Gpu'</span> <span class="operator">not</span> <span class="operator">in</span> type(x.op).__name__)
              <span class="keyword">for</span> x <span class="operator">in</span> f.maker.fgraph.toposort()]):
    print <span class="string">'Used the cpu'</span>
<span class="keyword">else</span>:
    print <span class="string">'Used the gpu'</span>
如果显示used <span class="operator">the</span> gpu说明theano在使用gpu加速
</code></pre></span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="John Yao" itemprop="image"/>
          <p class="site-author-name" itemprop="name">John Yao</p>
        </div>
        <p class="site-description motion-element" itemprop="description"></p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">4</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">3</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">5</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Yao</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"dubur"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
