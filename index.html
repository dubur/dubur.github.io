<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  <link href='//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>


<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.1"/>


    <meta name="description" content="Dubers Blog" />



  <meta name="keywords" content="Hexo,next" />





  <link rel="shorticon icon" type="image/x-icon" href="/favicon.ico?v=0.4.5.1" />


<meta name="description" content="Dubers Blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Duburs Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Duburs Blog">
<meta property="og:description" content="Dubers Blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Duburs Blog">
<meta name="twitter:description" content="Dubers Blog">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>

  <title> Duburs Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zsaf">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  

  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?de04554947ef5ddb1d9dd95ef6598c19";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  <div class="container one-column 
   page-home 
">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><h1 class="site-meta">
  <span class="logo-line-before"><i></i></span>
  <a href="/" class="brand" rel="start">
      <span class="logo">
        <i class="icon-next-logo"></i>
      </span>
      <span class="site-title">Duburs Blog</span>
  </a>
  <span class="logo-line-after"><i></i></span>
</h1>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon icon-next-home"></i> <br />
            首頁
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            <i class="menu-item-icon icon-next-categories"></i> <br />
            分類
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            <i class="menu-item-icon icon-next-about"></i> <br />
            關於
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            <i class="menu-item-icon icon-next-archives"></i> <br />
            歸檔
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            <i class="menu-item-icon icon-next-tags"></i> <br />
            標籤
          </a>
        </li>
      

      
      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div id="content" class="content"> 
  <section id="posts" class="posts-expand">
    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/10/02/Backproagation for CNN/" itemprop="url">
                BP for CNN
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-10-02T16:39:22+08:00" content="2015-10-02">
            2015-10-02
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/10/02/Backproagation for CNN/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/10/02/Backproagation for CNN/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>对于所有从事机器学习工作的研究者及工程师而言，BP算法是最熟悉不过的了，然而自从deep learning成为研究热点之后，为了解决不同种类的问题，神经网络的结构愈发复杂起来，面对图像信息的提取与图像特征的学习，CNN已经成为众多神经网络的佼佼者，由于CNN与普通网络的区别，训练的BP算法也需要进行相应的调整，为此笔者查阅了一些资料，现将CNN的BP算法整理如下</p>
<h2 id="features-of-cnn">Features of CNN</h2>
<h3 id="convolutional-layer">Convolutional layer</h3>
<p>卷积层引入了卷积核的概念，输入向量可以理解为是卷积核与输入向量卷积产生的结果，因此并不是全连接的关系，在这里我们可以回忆一下卷积运算的定义：卷积是通过两个函数f和g生成另一个函数的计算方法，其计算公式为：</p>
<p><span class="math">\[(f*g)[n] = \sum_{m=-\infty}^\infty f[n-m] \times g[m]\]</span></p>
<p>直观一点就是将函数f绕y轴翻转从左向右平移，计算f与g的重叠部分的面积，如下图（Wiki）<img src="/img/BP_for_CNN/Wiki-BoxConvAnim.gif" alt="img"></p>
<p>因此在CNN的卷积运算中，我们可以认为将卷积核翻转在原始图像上进行滑动，计算重叠部分的elementwise的乘积并求和，用数学公式表达就是： <span class="math">\[x_j^l = f(\sum_{i \in M_j} x_i^{l-1} * k_{ij}^l + b_j^l)\]</span></p>
<p>式中j表示第j个feature map，在这里仅仅是为了完整性才加上j，为了理解卷积层的反向传播，我们假设feature map仅仅有一个，即卷积核的数量为1</p>
<h3 id="pooling-layer">Pooling layer</h3>
<p>池化层比卷积层来得简单，无论是MaxPooling还是Average Pooling我们很容易找到输入与输出向量的关系，需要注意的是对于MaxPooling，计算最大值的同时需要记住该最大值的下标，在反向传播时需要用到该下标。</p>
<h2 id="backpropagation">BackPropagation</h2>
<p>按照BP算法的流程， 我们只要知道这一节中我们假设BP算法已经反向传播到最后一个全连接层，记为<span class="math">\(\delta^l\)</span> ### Pooling layer 先从比较简单的池化层入手，注意到输入向量在经过池化层之后，假设原始图像尺寸为H*W, 窗口尺寸为d，且窗口滑动不重叠，则输出向量的尺寸为：</p>
<pre><code>H*W -&gt; Ceil(H/d) * Ceil(W/d)</code></pre>
<p>由于Pooling层没有权值函数，仅仅是压缩了原始向量的尺寸，因此在进行反向传播时，也只需要将残差delta“放大”到原来的大小就可以了，pooling的数学表达式及其偏导关系为：</p>
<p><span class="math">\[g(x)= \begin{cases}\sum_{k=1}^mx_k \over m, \ &amp; {\partial g \over \partial x} = {1 \over m } \  mean \ pooling
     \cr max(x) &amp; {\partial g \over \partial x} = {\begin{cases} 1 &amp;if \ x_j = max(x)
\cr  0 &amp;otherwise \ max \ pooling
\end{cases}} \end{cases}\]</span></p>
<p>pooling的BP算法，看上去复杂其实原理非常简单，我们只需要进行一步上采样upsampling的操作，对于maxpooling，我们将输出向量的每个元素扩展成d*d的矩阵，前向传播时纪录的下标所对应的元素为该元素的值，其他置为0，而对于meanpooling，我们只需要将矩阵中元素都置为该元素的值就可以了</p>
<p>之所以被称作upsampling就是为了对应前向传播时我们使用降采样downsampling，记作<span class="math">\(up(x)\)</span></p>
<p>因此残差在经过pooling层时，我们可以简单地计算上一层残差为</p>
<p><span class="math">\[\delta^{l-1} = f^{&#39;}(u_j^l \circ up(\delta_j^{l+1}))\]</span></p>
<p>式中f表示pooling层后接的激活函数，u表示该f的输入，但是很多的CNN架构中Pooling层后面不经过激活函数，因此一般这一项为1，<span class="math">\(\circ\)</span>指elementwise乘法</p>
<h3 id="convolution-layer">Convolution layer</h3>
<p>其实在卷积层我们也用到了池化压缩的思想，但是卷积层有卷积核，相当于全连接层的权值矩阵，因此我们可以类比fc层的残差计算方法：</p>
<p><span class="math">\[\delta^{l-1} = (W^T \delta^{l}) * f^{&#39;}(u^{l-1})\]</span></p>
<p>卷积层不像全连接层那样输入与输出之间每两个元素都有联系，对于输出向量中的每个元素，在输入向量中与之相关联的元素只有d*d个，即卷积窗口的大小，为了简单我们以一维向量的卷积为例，我们很容易通过下图看出对应关系：<img src="/img/BP_for_CNN/RiverTrain-ImageConvDiagram.png" alt="kernel"></p>
<p>因此反向传播时，我们也只需要考虑部分的对应关系:<img src="/img/BP_for_CNN/BP_conv_layer.png" alt="img"></p>
<p><span class="math">\[\delta_n^{l-1} = {\partial J \over \partial x_n} = {\partial J \over \partial y}{\partial y \over \partial x_n} = \sum_{i=1}^{|w|} {\partial J \over \partial y_{n-i+1}}{\partial y_{n-i+1} \over {\partial x_n}} = \sum_{i=1}^{|w|} {\delta_{n-i+1}^{l}w_i} = (\delta^{l}*flip(w))[n]\]</span></p>
<p>式中w代表卷积核的参数，上式的结果可以简化为：</p>
<p><span class="math">\[\delta^{l-1} = \delta^{l} * flip(w)\]</span> 这里我们忽略了激活函数f，可以看出该公式与全连接层的公式十分类似</p>
<p>式中flip表示翻转，意思是在反向传播时需要将卷积核进行翻转 这里其实有一个问题，在标准的卷积运算中需要翻转因此在反向传播时也需要翻转，那么如果我们不进行翻转的话，在BP过程中就不需要翻转，我们可以将其视作为全连接层的退化版本(去掉了大部分的连接关系) 事实上，由于卷积层训练的就是卷积核的参数，因此我们不翻转同样可以训练，而且可以简化计算过程</p>
<p>残差的反向传播文明已经完成，下面就是通过残差计算偏导了，原本全连接层在计算偏导时只需要将残差乘以对应层的向量即可，但是卷积层是部分连通，不需要所有元素参与计算，所以我们需要修改一下：</p>
<p><span class="math">\[{\partial J \over \partial {k_{ij}^{l}}} = \sum_{u,v} ({\delta_{j}^{l}}_{uv}(p_{i}^{l-1})_{uv})\]</span></p>
<p>可以看出与fc层的区别也只是修改了参与计算的元素而已</p>
<p>式中p表示的是原输入向量中与卷积核元素<span class="math">\(k_{ij}\)</span>相乘的元素，如下图所示：<img src="/img/BP_for_CNN/kernel_partial.jpg" alt="kernel_partial"></p>
<p>假设步长为2，那么<span class="math">\(p_i\)</span>指的就是图中红色的部分，如果步长设置为1，那么<span class="math">\(p_i\)</span>指的就是图像中阴影加粗的部分</p>
<h2 id="conclusion">Conclusion</h2>
<p>经过分析我们可以得出CNN的反向传播其实也有规律可循，关键是需要了解BP算法的原理，这是理清任何使用BP算法进行优化求解的神经网络的基本，抓住两个点：反向传播其实是将输出层作为输入层计算各层残差的过程，把握好各层的性质理解起来就比较容易了。</p>
<h2 id="reference">Reference</h2>
<p>Bouvrie J. Notes on convolutional neural networks[J]. 2006.</p>
<p>http://www.slideshare.net/kuwajima/cnnbp</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/24/Caffe-源码阅读笔记一（基本代码架构）/" itemprop="url">
                Caffe 源码阅读笔记一（基本代码框架）
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-24T22:11:07+08:00" content="2015-08-24">
            2015-08-24
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/caffe/" itemprop="url" rel="index">
                  <span itemprop="name">caffe</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/24/Caffe-源码阅读笔记一（基本代码架构）/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/24/Caffe-源码阅读笔记一（基本代码架构）/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>在开始阅读Blob之前，需要预先了解Caffe的架构与protobuf，否则看代码的时候容易一头雾水</p>
<h3 id="caffe基本架构">Caffe基本架构</h3>
<p>对于任何一个网络而言，Caffe主要用4个类来构造网络：</p>
<ul>
<li>BLob:存储数据，blob既可以存储网络的权重，在进行训练时修改的其实就是每个blob实例</li>
<li>Layer:构成网络的基础单元，无论是卷积层，pooling层还是全连接层等等都是从这个类派生出来的</li>
<li>Net:定义一个网络的架构，当我们把网络中每一层都建立完之后，通过Net类来搭建整个网络</li>
<li>Solver:用来训练网络的类，例如SGD就在这部分实现</li>
</ul>
<h3 id="protobuf">Protobuf</h3>
<p>在理清基本架构之后可能有人会问：一个网络需要那么多参数，且每个参数的数据类型不尽相同，如何解决这个问题呢？</p>
<p>Protobuf就是用来解决此类问题的，我们只需要像写配置文件一样把网络中每一层的参数设置好(当然配置文件必须按照一定的格式)，protobuf会自动生成代码，用来解析“配置文件”中的参数结构体，代码中包括了对参数的设置、读取和序列化等等操作 另外当网络训练完成之后，存储时也使用了protobuf进行处理，把Net转化为多个Message对象进行存储</p>
<p>Caffe代码中参数初始化和参数操作都是基于protobuf完成，只需要定义网络参数，不再需要考虑如何把参数传递给应用程序的问题了，protobuf会帮你完成～</p>
<h3 id="blob">Blob</h3>
<p>Blob本身其实没有特别多的内容，作为Caffe中数据的基础单元，我们可以简单地吧Blob理解成一个容器，里面存储的是多维向量及其相关的信息</p>
<p>首先看Blob.hpp:</p>
<pre><code>Blob()
 : data_(), diff_(), count_(0), capacity_(0) {}
   </code></pre>
<p>构造函数中初始化了4个对象，data_表示Blob中的数据，diff_表示反向传播时的误差，count_表示数据当前的维度(因为可以reshape)，而capacity_表示数据最大的维度</p>
<p>接下来在Blob.cpp里主要包含了实例的初始化和reshape函数，值得注意的是如果blob中存储的是网络的参数(如全连接层和卷积层的kernel)，那么Blob提供update函数用来更新自己的参数。</p>
<pre><code>void Blob&lt;Dtype&gt;::Update() {                                                                                                                                  
  // We will perform update based on where the data is located.                                                                                               
  switch (data_-&gt;head()) {                                                                                                                                    
  case SyncedMemory::HEAD_AT_CPU:                                                                                                                             
    // perform computation on CPU                                                                                                                             
    caffe_axpy&lt;Dtype&gt;(count_, Dtype(-1),                                                                                                                      
      static_cast&lt;const Dtype*&gt;(diff_-&gt;cpu_data()),                                                                                                         
      static_cast&lt;Dtype*&gt;(data_-&gt;mutable_cpu_data()));                                                                                                      
    break;                                                                                                                                                    
  case SyncedMemory::HEAD_AT_GPU:                                                                                                                             
  case SyncedMemory::SYNCED:                                                                                                                                  
  #ifndef CPU_ONLY                                                                                                                                              
  // perform computation on GPU                                                                                                                             
  caffe_gpu_axpy&lt;Dtype&gt;(count_, Dtype(-1),                                                                                                                  
      static_cast&lt;const Dtype*&gt;(diff_-&gt;gpu_data()),                                                                                                         
      static_cast&lt;Dtype*&gt;(data_-&gt;mutable_gpu_data()));                                                                                                      
  #else                                                                                                                                                         
  NO_GPU;                                                                                                                                                   
  #endif                                                                                                                                                        
  break;                                                                                                                                                    
  default:                                                                                                                                                    
    LOG(FATAL) &lt;&lt; &quot;Syncedmem not initialized.&quot;;                                                                                                               
  }                                                                                                                                                           
}                            </code></pre>
<p>代码中包含了GPU和CPU两种实现，其实核心就是caffe_gpu_axpy那一句更新参数</p>
<h3 id="layer">Layer</h3>
<p>对Blob有了一定了解之后可以看Layer了，事实上Layer是caffe架构中内容最多的部分，我们在写配置文件的时候其实都是在组合layer构成一个网络，很多运算操作在caffe中都是以layer的形式存在的，如argmax和elementwise运算等等.许多对caffe的扩展其实也是继承Layer层重新定义了具有新功能的层</p>
<p>layer的工作模式类似数学中的函数概念，给定一个输入(bottom blobs),layer内部完成自己的功能，返回一个输出(top blobs)</p>
<p>caffe中的layer分两种，common layer和vision layer: - data_layers.hpp中声明了神经网络与输入数据之间的交互层，如导入/导出hdf5数据，从图像中导入数据等等 - common_layers.hpp中声明了许多常用的包含基础功能(flatten, softmax等)的层 - vision_layers.hpp中主要包含对针对图像处理的层(convolutional, pooling等) - neuron_layers.hpp中主要包含与神经元的定义与操作相关的层(dropout,ReLu等) - loss_layers.hpp中主要包含了计算网络Loss的层，除了我们常见的softmax loss之外还包括了比较常用多euclidean loss, hinge loss等等 看了下vision layer中居然还包含了SPPnet这种处理image scale的层，除此之外还有deconvolution layer,确实是很给力啊</p>
<p>caffe还提供了layer_factory供其他人快速实现自己的layer</p>
<p>所有的Layer层都包含Forward和Backward的函数，这两个函数包含了整个layer的计算功能，对于开发者而言如果想实现自己的layer，其实主要就是完成这两个函数逻辑</p>
<p>具体的某一种Layer的代码解析不在本文的范畴里，以后会另开blog详细描述常用的layer代码实现</p>
<h3 id="net">Net</h3>
<p>Net中包含了对整个网络的操作实现，在看源代码之前笔者只想到了设置learning_rate、 weight_decay和迭代次数等等功能，以及控制Forward和Backward等，然而事实上caffe提供了许多非常有用的函数，如在RCNN training中提到的share weights功能，使得你可以直接使用预先训练好的网络参数进行初始化，这也是为什么在caffe的example中我们可以直接通过命令行对imagenet训练出来的网络进行fine－tuning的原因</p>
<h3 id="solver">Solver</h3>
<p>相对而言solver的实现比较简单，创建solver对象时我们只需要关注优化算法就可以了，caffe目前提供三种梯度下降的方法： - SGD(momentum) - NesterRov - Adagrad</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/24/Linix链接原理(一)副本 2/" itemprop="url">
                Linix系统链接原理(一)
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-24T22:11:07+08:00" content="2015-08-24">
            2015-08-24
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/OS/" itemprop="url" rel="index">
                  <span itemprop="name">OS</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/24/Linix链接原理(一)副本 2/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/24/Linix链接原理(一)副本 2/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="computer-systems-linking">Computer Systems: Linking</h2>
<p>《深入理解计算机系统》中Linking这一章对编译过程中链接器的作用做了非常详尽的介绍，对于程序员而言可能在算法、数据结构以及编程语言上花的时间比较多，然而对于计算机如何将代码编译为可执行文件的过程缺乏系统的了解，然而如果需要了解Linux下程序完整的编译执行过程，阅读本书的相关章节是十分必要的，笔记比较长所以分成两部分</p>
<h3 id="基本概念">基本概念</h3>
<p>Linking的定义：<strong>将各种代码和数据部分收集起来并组合成一个单一文件的过程</strong>，要理解这句话的含义首先需要明白GNU系统的一般编译流程：<img src="/img/Linux/compile_process.png" alt="compile"></p>
<p>当然链接这一步并不一定要在程序编译时进行，Linking操作的对象是已经编译完成的目标文件，输出的结果就是可执行文件，链接器在整个编译过程中起到以下两个作用:</p>
<pre><code>1.解析.o文件将符号引用与定义联系起来
2.为指令与变量分配运行时地址</code></pre>
<h3 id="可重定位目标文件">可重定位目标文件</h3>
<p>要了解链接器的工作原理首先需要清楚它的输入文件，.o文件</p>
<p>在弄清可重定位目标文件的内容之前，一些平时不好的习惯可能会引发概念上的混淆 比如在Linux上编译程序时，我们可能会执行这样的指令：</p>
<pre><code>gcc -o *main.o* -I(dirs for header files) -l(paths for libraries)</code></pre>
<p>之后可以直接在控制台执行./main.o，这样看来我们似乎直接执行了可重定位目标文件，然而这条指令中的.o文件是编译程序输出的最后结果，也就是说我们把可执行文件的名字命名成了main.o，而实际上汇编器生成的.o文件位于tmp目录下，因此在实际写程序时最好把文件的名字区分开，以免混淆。</p>
<h4 id="文件格式">文件格式</h4>
<p>书中可重定位目标文件的文件内容非常繁杂，简单而言该文件分为许多的section，每个section都包含了，总结下来分为以下三类</p>
<pre><code>1.存储机器指令与全局变量(如.text, .data, .bss)
2.存储指令与变量的位置相关信息（即重定位信息, 如.rel.text, rel.data）
3.存储函数与变量的各种属性(.symtab)</code></pre>
<p>需要注意的是链接器仅仅考虑全局变量，函数中的局部变量由栈管理</p>
<h4 id="符号表.symtab">符号表.symtab</h4>
<h5 id="符号">符号</h5>
<p>从程序员的角度看符号是一个个的变量名和函数名，而对于链接器而言只是一些字符串而已，因此在这里被称作符号，符号表包含了原文件中各个符号(函数和变量)的信息，有一种符号不在链接器的管理范围之内，符号表不包含对应于<strong><em>本地非静态程序变量的符号</em></strong>，因为这些符号也由栈管理</p>
<p>需要注意的是当变量名前面指定了static关键字之后，无论该变量定义在函数内部还是外部，都会存储在.o文件的符号表中，对于函数也同样如此。</p>
<h5 id="符号表的格式">符号表的格式</h5>
<p>如果从数据库的角度看，符号表可以看作是一张普通的关系表，每个符号及其相关属性都可以被看作是一个元祖，整张表中包含的属性包括：</p>
<pre><code>符号的偏移量(存储格式为int型，可以理解为下标)
符号的地址
符号的类型（数据或者是函数）
符号的大小
 </code></pre>
<p>书中给出的例子非常清楚，以main.o为例，基本的格式如下图：<img src="/img/Linux/symtab.png" alt="symtab"></p>
<p>附上源码：</p>
<pre><code>/* main.c */ 
void swap();

int buf[2] = {1,2}

int main(){
    swap();
    return 0;
}</code></pre>
<h3 id="符号解析">符号解析</h3>
<h4 id="全局变量解析">全局变量解析</h4>
<p>之前提到链接器的作用之一就是解析.o文件，其实主要解析的就是符号表中的符号，实际上链接器在这一步所做的工作是将原文件出现的变量的引用和符号表中的符号定义联系起来</p>
<p>如果符号的定义就在原文件中，那么链接器可以直接联系引用和定义，但是如果符号不是定义在原文件中，链接器假设该符号定义在其他某个模块中，同时如果出现多个文件同时定义相同变量的情况，比如一个文件中声明全局变量，另一个文件定义全局变量</p>
<ul>
<li>强符号：已初始化的函数及变量</li>
<li>弱符号：未初始化的函数及变量</li>
</ul>
<p>当出现多个同名的全局变量时，链接器遵循以下三个标准进行解析： * 不允许有多个强符号 * 如果有一个强符号和多个弱符号，那么选择强符号 * 如果有多个弱符号，那么从这些弱符号中任意选择一个</p>
<p>简单而言链接器不允许多重定义，并且会优先连接变量定义的位置</p>
<p>通过对符号解析部分的介绍我们也明白了为什么写程序时不鼓励定义太多的全局变量的原因，当程序架构比较大，每个人分别编写自己的模块时，很容易发生全局变量的定义冲突，而且编译器不会报这样的错误，因为已经涉及到代码整合时的程序逻辑层面的错误而不是编程语言规范的错误。</p>
<h4 id="与静态库连接">与静态库连接</h4>
<p>在阅读此章节之前我认为静态库在编译时进行链接，而动态链接库在运行时进行链接，这种理解其实并不正确，这两种库的不同之处不在链接的时机而在于它们构成和设计思想</p>
<h5 id="静态库">静态库</h5>
<p>由于标准函数会被广泛使用，如果在编译时直接生成这些函数的代码不仅增加编译器的工作量，同时当这些函数更新时也难以维护编译器，如果为每个函数都生成一个.o文件，那么我们需要写很冗长的链接命令，同时也会消耗多余的存储空间，费时也费力。</p>
<p>静态库可以很好地解决链接这些“常用函数”的问题，一个静态库以一种称为存档的特殊文件格式存放在磁盘中，可以理解为是一组.o文件的集合。</p>
<p>直观上讲，当我们需要链接某个函数时，我们只需要链接该静态库中的对应模块，即该函数对应的.o文件就可以了，因此静态库只需要向链接器提供每个模块的大小和位置就可以了。</p>
<p>对于自己原先的对于静态库的理解，当编译时添加-static参数后，链接器会生成一个完全链接的可执行目标文件，也就是在编译时进行链接，这样生成的可执行文件可以直接在内存中执行，因此链接的时机和链接的库的类型其实并无多大关系</p>
<h5 id="使用静态库解析引用">使用静态库解析引用</h5>
<p>回到符号解析，之前介绍了解析符号时，符号定义在原文件中的情况，那么如果符号定义在静态库中怎么进行链接呢？</p>
<p>首先链接器会按顺序扫描.o文件和静态链接库，在扫描过程中链接器会维护一个.o文件的集合E, 一个未解析的符号集合U,以及一个在输入文件中已定义的符号集合D，链接时按照以下流程进行链接：<img src="/img/Linux/compile.png" alt="compile"></p>
<p>按照笔者理解U和D可以认为是两个数组，U用来存储未匹配到定义的符号，而D存储已定义的符号，而E用来存储所有输入文件中链接器对符号的解析结果</p>
<p>需要注意的链接器只会链接在U中的符号，由于链接器是按顺序扫描的，因此如果符号的定义出现在引用之前并且引用后再也没有出现该符号的定义，那么就会出现扫描结束后U不为空的情况，导致链接出错</p>
<h3 id="重定位">重定位</h3>
<p>完成符号解析之后，代码节与数据节的大小就已经确定，之前提到链接器的第二个作用就是分配运行时地址，重定位的目的是为了让CPU知道符号在存储器中的位置，因此重定位分为两步：</p>
<pre><code>1.将所有输入模块的同类型节(如data节)合并，并将运行时的地址赋给这些合并后的“聚合节”，同时赋给输入模块的每个节
2.修改代码节和数据节中对每个符号的引用，使得他们指向正确的地址</code></pre>
<p>这样写可能还是不够清晰，简单而言第一步为每个节分配运行时地址，而第二步为节中每个指令或数据分配运行时地址，而为了给指令与数据分配地址，我们需要明白<em>重定位条目</em>的概念</p>
<pre><code>`重定位条目`

在介绍可重定向目标文件的内容时提到过有section负责存储重定位信息，如代码的重定位信息在.rel.text中，数据的重定位信息在.rel.data中，这两个张表告诉链接器需要修改的引用在节中的什么位置以及重定位的类型(如何修改引用的地址)，表中每一项就是一个重定位条目，当链接器找到节中的符号后根据重定位类型计算该符号运行时地址。</code></pre>
<p>书中介绍了两种重定位的类型：PC相对引用与绝对引用</p>
<h4 id="pc相对引用">PC相对引用</h4>
<p>相对引用使用基地址加偏移量的方式寻址，基地址存储在程序计数器PC中，所以我们只需要计算出偏移量，CPU就可以找到该指令或者数据的运行时地址了，假设我们已经知道了节的存储器地址和符号的运行时地址，计算方式如下：</p>
<pre><code>计算swap在节中的地址 refptr ＝ s＋r.offset
计算引用swap的指令地址： refaddr = ADDR(s) + r.offset
计算偏移量  *refptr = ADDR(r.symbol) + *refptr - refaddr</code></pre>
<p>书中给的例子比较难懂，首先抛开那个 *refptr不看， 参考书3.6节的内容，在进行PC相对寻址时，目标跳转地址就是例子中swap函数的地址，而链接器需要修改的就是引用swap的指令的目标编码，这两者之间的关系为：</p>
<pre><code>目标编码 ＝ 目标跳转地址 － PC</code></pre>
<p>因此在求目标编码时，实际的目标跳转地址等于swap的地址：0x80483c8,从3.6节的例子中我们可以看出PC的值是在当前指令的地址基础上＋4，即当前指令的后面一条指令，因此实际上原式等于</p>
<pre><code>目标编码 ＝ 目标跳转地址 － （当前指令地址＋4）</code></pre>
<p>当前指令自然是引用swap的指令地址，该地址可以用节基地址加偏移的方式求得:</p>
<pre><code>引用swap的指令地址 ＝ 节基地址 ＋ 引用偏移 </code></pre>
<h4 id="绝对引用">绝对引用</h4>
<p>绝对引用理解起来就容易许多，修改引用时直接把值修改为目标跳转地址和初始值的和，CPU会直接通过该引用内的值查找对应的数据</p>
<p>按照书中的例子，bufp0需要重定位指向数组buf的首地址，又由于原本.data节中的重定位条目中的初始值为0，所以直接把buf数组的首地址赋给bufp0即可。由于重定位的对象是指针，因此该过程类似于为指针赋值的过程。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/24/Linux链接原理(二)/" itemprop="url">
                Linix系统链接原理(二)
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-24T22:11:07+08:00" content="2015-08-24">
            2015-08-24
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/OS/" itemprop="url" rel="index">
                  <span itemprop="name">OS</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/24/Linux链接原理(二)/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/24/Linux链接原理(二)/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="computer-systems-linking">Computer Systems: Linking</h2>
<p>《深入理解计算机系统》中Linking这一章对编译过程中链接器的作用做了非常详尽的介绍，对于程序员而言可能在算法、数据结构以及编程语言上花的时间比较多，然而对于计算机如何将代码编译为可执行文件的过程缺乏系统的了解，然而如果需要了解Linux下程序完整的编译执行过程，阅读本书的相关章节是十分必要的，笔记比较长所以分成两部分</p>
<h3 id="可执行目标文件">可执行目标文件</h3>
<p>可执行目标文件与前面提到的可重定向目标文件最大的区别在于，可执行目标文件必须将文件中的节映射到存储器中的段中，同时它还定义了一个.init函数用来初始化代码，由于可执行目标文件可以直接运行，因此该文件中不包含重定向信息</p>
<h4 id="可执行目标文件的加载">可执行目标文件的加载</h4>
<p>将可执行目标文件中的代码和数据复制到存储器的过程称为<strong>加载</strong>,存储器映像为程序分配了的数据空间(包括堆栈)以及用来存储数据和代码的段，基本结构如下：</p>
<pre><code>最上方是用户栈，主要用来存储局部变量等
下面的存储器区域用来存储共享库
再下面是堆空间(malloc时就会分配这部分空间)
最后是可执行文件中的数据和代码段</code></pre>
<p>比较令我印象深刻的是这一章节介绍了C程序的启动细节，当加载器完成代码和数据段的加载后：</p>
<pre><code>加载器跳转到程序入口点，即符号_start的地址，在该地址启动代码
调用.text和.init的初始化例程，启动代码调用atexit例程，该程序包含原程序正常终止时应该调用的程序
之后运行原程序中的main函数执行我们的C代码
最后exit函数调用_exit返回</code></pre>
<p>从启动例程的伪代码中就可以看出main函数在C程序中必须出现，这也就是为什么我们把main函数称为程序入口的原因</p>
<p>对于习题7.5中第二个问题，无论直接调用exit还是使用return，抑或是什么都不写，最后都会调用_exit程序把控制返回给操作系统</p>
<h3 id="动态链接共享库">动态链接共享库</h3>
<p>静态库虽然能很好地管理大量公共函数的链接问题，但是如果有多个程序同时引用了一个函数，函数会被复制多份，显然这会占用多余的内存空间，因此<strong>共享库</strong>被设计来解决函数代码重复复制的问题</p>
<p>按照书上的说法，在运行时共享库可以加载到任意的存储器地址并和存储器中的程序链接起来，这个过程也被称为<strong>动态链接</strong>，有专门的动态链接器完成此任务，共享库独立于应用程序，因此在链接时链接器不会拷贝动态链接库中的代码和数据</p>
<p>在加载时，如果需要链接动态链接库，那么加载器会把控制权转交给动态链接器，动态链接器会执行如下重定位流程：</p>
<pre><code>1.重定位每个动态链接库的文本和数据到存储器段
2.重定位可执行目标文件中的符号和引用</code></pre>
<p>那么如何做到让多个进程共享一块存储器中的库代码呢？</p>
<p>书中介绍了编译库代码，使得链接器不需要修改代码就可以在任何地址加载和执行，相当于预先把库代码编译完成后，这里我们可以看出动态链接库与静态链接库的一个重要的不同点：动态链接库不需要链接器进行重定位，但是按照之前几节的说法，如果我们需要调用外部定义的数据或者函数，那么如何找到对应的地址呢？</p>
<p>书中提到了关键的一个事实：<strong>无论我们在存储器的何处加载目标模块，数据段总是被分配成紧随在代码段的后面</strong>这意味着如果代码段和数据段的长度固定，那么我们始终可以通过相对寻址找到我们需要的数据或指令。</p>
<h4 id="全局偏移量表got">全局偏移量表GOT</h4>
<p>由于动态链接库中的代码不会编译到可执行目标文件中，因此我们需要给可执行目标文件一定的信息让加载起能够找到动态链接库的代码，因此引入GOT和PLT两张表，分别用来记录在这个目标模块中引用的全局变量和外部定义函数。</p>
<ul>
<li><p>通过GOT引用全局变量： 当程序需要引用某个全局变量时：</p>
<pre><code>首先将PC的值移到寄存器中
接着通过变量在GOT中的偏移找到存有该变量地址的条目
最后通过条目中提供的全局地址找到该变量</code></pre></li>
</ul>
<p>从C/C++程序员的角度而言，这相当于进行两次寻址，第一次找到全局变量的地址，第二次找到该变量，GOT就是一张提供了各变量地址的表 类似地，当程序需要调用某个外部函数时，同样执行上述的步骤</p>
<p>值得注意的是为了防止每次调用动态链接库的数据或函数时都要重复上述的步骤，编译器使用了一种称为“延迟绑定”的技术</p>
<h4 id="过程链接表plt">过程链接表PLT</h4>
<p>介绍延迟绑定之前，首先需要介绍过程链接表的概念，书中没有详细说明PLT存储的内容，因此我查阅了一些资料：</p>
<pre><code>过程链接表用于把位置独立的函数调用重定向到绝对位置。PLT中的每个条目为本程序要调用的函数提供一个入口，PLT 的第1个入口PLT[0] 是一段访问动态链接器的特殊代码。程序对PLT入口的第1次访问都转到了PLT[0]</code></pre>
<p>当第一次过程调用发生时，最后总会跳转到PLT[0]中，<strong>PLT[0]包含跳转到动态链接器的信息</strong>，待完成符号解析后，将符号的实际地址存入相应的GOT项，这样以后调用函数时可直接跳到实际的函数地址，不必再执行符号解析函数。延迟绑定指的就是在第一次引用动态链接库的指令时，通过PLT和GOT两张表寻找运行时的指令地址，之后修改GOT项的内容使得后面的调用不再需要重复相同的步骤。</p>
<p>Linux用一个全局的库映射信息结构struct link_map链表来管理和控制所有动态库的加载，动态库的加载过程实际上是映射库文件到内存中，并填充库映射信息结构添加到链表中的过程。觉得这部分书中讲得也不够清楚，还是有点似懂非懂。</p>
<p>但是读完动态链接的部分之后，觉得动态链接与链接的定义其实有点出入，按照本章开头的定义，链接强调组合成为单一文件，然而动态链接的本质却是通过间接的方式寻找调用的数据和代码，链接器在链接动态库时紧紧拷贝重定位和符号表信息，并不参与真正的链接过程，因此我认为链接与动态链接是两个并列的概念</p>
<h3 id="总结">总结</h3>
<p>总的来说，综合<strong>符号解析</strong>、<strong>静态库链接</strong>和<strong>动态链接</strong>，一个完整的链接过程如下图：<img src="/img/Linux/entire_process.png" alt="whole_process"></p>
<p>总体而言链接的重点在于重定位与两类链接库的链接原理，理清这些概念之后整个链接的过程就比较清晰了。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/24/Linux链接原理(一)/" itemprop="url">
                Linix系统链接原理(一)
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-24T22:11:07+08:00" content="2015-08-24">
            2015-08-24
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/OS/" itemprop="url" rel="index">
                  <span itemprop="name">OS</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/24/Linux链接原理(一)/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/24/Linux链接原理(一)/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><h2 id="computer-systems-linking">Computer Systems: Linking</h2>
<p>《深入理解计算机系统》中Linking这一章对编译过程中链接器的作用做了非常详尽的介绍，对于程序员而言可能在算法、数据结构以及编程语言上花的时间比较多，然而对于计算机如何将代码编译为可执行文件的过程缺乏系统的了解，然而如果需要了解Linux下程序完整的编译执行过程，阅读本书的相关章节是十分必要的，笔记比较长所以分成两部分</p>
<h3 id="基本概念">基本概念</h3>
<p>Linking的定义：<strong>将各种代码和数据部分收集起来并组合成一个单一文件的过程</strong>，要理解这句话的含义首先需要明白GNU系统的一般编译流程：<img src="/img/Linux/compile_process.png" alt="compile"></p>
<p>当然链接这一步并不一定要在程序编译时进行，Linking操作的对象是已经编译完成的目标文件，输出的结果就是可执行文件，链接器在整个编译过程中起到以下两个作用:</p>
<pre><code>1.解析.o文件将符号引用与定义联系起来
2.为指令与变量分配运行时地址</code></pre>
<h3 id="可重定位目标文件">可重定位目标文件</h3>
<p>要了解链接器的工作原理首先需要清楚它的输入文件，.o文件</p>
<p>在弄清可重定位目标文件的内容之前，一些平时不好的习惯可能会引发概念上的混淆 比如在Linux上编译程序时，我们可能会执行这样的指令：</p>
<pre><code>gcc -o *main.o* -I(dirs for header files) -l(paths for libraries)</code></pre>
<p>之后可以直接在控制台执行./main.o，这样看来我们似乎直接执行了可重定位目标文件，然而这条指令中的.o文件是编译程序输出的最后结果，也就是说我们把可执行文件的名字命名成了main.o，而实际上汇编器生成的.o文件位于tmp目录下，因此在实际写程序时最好把文件的名字区分开，以免混淆。</p>
<h4 id="文件格式">文件格式</h4>
<p>书中可重定位目标文件的文件内容非常繁杂，简单而言该文件分为许多的section，每个section都包含了，总结下来分为以下三类</p>
<pre><code>1.存储机器指令与全局变量(如.text, .data, .bss)
2.存储指令与变量的位置相关信息（即重定位信息, 如.rel.text, rel.data）
3.存储函数与变量的各种属性(.symtab)</code></pre>
<p>需要注意的是链接器仅仅考虑全局变量，函数中的局部变量由栈管理</p>
<h4 id="符号表.symtab">符号表.symtab</h4>
<h5 id="符号">符号</h5>
<p>从程序员的角度看符号是一个个的变量名和函数名，而对于链接器而言只是一些字符串而已，因此在这里被称作符号，符号表包含了原文件中各个符号(函数和变量)的信息，有一种符号不在链接器的管理范围之内，符号表不包含对应于<strong><em>本地非静态程序变量的符号</em></strong>，因为这些符号也由栈管理</p>
<p>需要注意的是当变量名前面指定了static关键字之后，无论该变量定义在函数内部还是外部，都会存储在.o文件的符号表中，对于函数也同样如此。</p>
<h5 id="符号表的格式">符号表的格式</h5>
<p>如果从数据库的角度看，符号表可以看作是一张普通的关系表，每个符号及其相关属性都可以被看作是一个元祖，整张表中包含的属性包括：</p>
<pre><code>符号的偏移量(存储格式为int型，可以理解为下标)
符号的地址
符号的类型（数据或者是函数）
符号的大小
 </code></pre>
<p>书中给出的例子非常清楚，以main.o为例，基本的格式如下图：<img src="/img/Linux/symtab.png" alt="symtab"></p>
<p>附上源码：</p>
<pre><code>/* main.c */ 
void swap();

int buf[2] = {1,2}

int main(){
    swap();
    return 0;
}</code></pre>
<h3 id="符号解析">符号解析</h3>
<h4 id="全局变量解析">全局变量解析</h4>
<p>之前提到链接器的作用之一就是解析.o文件，其实主要解析的就是符号表中的符号，实际上链接器在这一步所做的工作是将原文件出现的变量的引用和符号表中的符号定义联系起来</p>
<p>如果符号的定义就在原文件中，那么链接器可以直接联系引用和定义，但是如果符号不是定义在原文件中，链接器假设该符号定义在其他某个模块中，同时如果出现多个文件同时定义相同变量的情况，比如一个文件中声明全局变量，另一个文件定义全局变量</p>
<ul>
<li>强符号：已初始化的函数及变量</li>
<li>弱符号：未初始化的函数及变量</li>
</ul>
<p>当出现多个同名的全局变量时，链接器遵循以下三个标准进行解析： * 不允许有多个强符号 * 如果有一个强符号和多个弱符号，那么选择强符号 * 如果有多个弱符号，那么从这些弱符号中任意选择一个</p>
<p>简单而言链接器不允许多重定义，并且会优先连接变量定义的位置</p>
<p>通过对符号解析部分的介绍我们也明白了为什么写程序时不鼓励定义太多的全局变量的原因，当程序架构比较大，每个人分别编写自己的模块时，很容易发生全局变量的定义冲突，而且编译器不会报这样的错误，因为已经涉及到代码整合时的程序逻辑层面的错误而不是编程语言规范的错误。</p>
<h4 id="与静态库连接">与静态库连接</h4>
<p>在阅读此章节之前我认为静态库在编译时进行链接，而动态链接库在运行时进行链接，这种理解其实并不正确，这两种库的不同之处不在链接的时机而在于它们构成和设计思想</p>
<h5 id="静态库">静态库</h5>
<p>由于标准函数会被广泛使用，如果在编译时直接生成这些函数的代码不仅增加编译器的工作量，同时当这些函数更新时也难以维护编译器，如果为每个函数都生成一个.o文件，那么我们需要写很冗长的链接命令，同时也会消耗多余的存储空间，费时也费力。</p>
<p>静态库可以很好地解决链接这些“常用函数”的问题，一个静态库以一种称为存档的特殊文件格式存放在磁盘中，可以理解为是一组.o文件的集合。</p>
<p>直观上讲，当我们需要链接某个函数时，我们只需要链接该静态库中的对应模块，即该函数对应的.o文件就可以了，因此静态库只需要向链接器提供每个模块的大小和位置就可以了。</p>
<p>对于自己原先的对于静态库的理解，当编译时添加-static参数后，链接器会生成一个完全链接的可执行目标文件，也就是在编译时进行链接，这样生成的可执行文件可以直接在内存中执行，因此链接的时机和链接的库的类型其实并无多大关系</p>
<h5 id="使用静态库解析引用">使用静态库解析引用</h5>
<p>回到符号解析，之前介绍了解析符号时，符号定义在原文件中的情况，那么如果符号定义在静态库中怎么进行链接呢？</p>
<p>首先链接器会按顺序扫描.o文件和静态链接库，在扫描过程中链接器会维护一个.o文件的集合E, 一个未解析的符号集合U,以及一个在输入文件中已定义的符号集合D，链接时按照以下流程进行链接：<img src="/img/Linux/compile.png" alt="compile"></p>
<p>按照笔者理解U和D可以认为是两个数组，U用来存储未匹配到定义的符号，而D存储已定义的符号，而E用来存储所有输入文件中链接器对符号的解析结果</p>
<p>需要注意的链接器只会链接在U中的符号，由于链接器是按顺序扫描的，因此如果符号的定义出现在引用之前并且引用后再也没有出现该符号的定义，那么就会出现扫描结束后U不为空的情况，导致链接出错</p>
<h3 id="重定位">重定位</h3>
<p>完成符号解析之后，代码节与数据节的大小就已经确定，之前提到链接器的第二个作用就是分配运行时地址，重定位的目的是为了让CPU知道符号在存储器中的位置，因此重定位分为两步：</p>
<pre><code>1.将所有输入模块的同类型节(如data节)合并，并将运行时的地址赋给这些合并后的“聚合节”，同时赋给输入模块的每个节
2.修改代码节和数据节中对每个符号的引用，使得他们指向正确的地址</code></pre>
<p>这样写可能还是不够清晰，简单而言第一步为每个节分配运行时地址，而第二步为节中每个指令或数据分配运行时地址，而为了给指令与数据分配地址，我们需要明白<em>重定位条目</em>的概念</p>
<pre><code>`重定位条目`

在介绍可重定向目标文件的内容时提到过有section负责存储重定位信息，如代码的重定位信息在.rel.text中，数据的重定位信息在.rel.data中，这两个张表告诉链接器需要修改的引用在节中的什么位置以及重定位的类型(如何修改引用的地址)，表中每一项就是一个重定位条目，当链接器找到节中的符号后根据重定位类型计算该符号运行时地址。</code></pre>
<p>书中介绍了两种重定位的类型：PC相对引用与绝对引用</p>
<h4 id="pc相对引用">PC相对引用</h4>
<p>相对引用使用基地址加偏移量的方式寻址，基地址存储在程序计数器PC中，所以我们只需要计算出偏移量，CPU就可以找到该指令或者数据的运行时地址了，假设我们已经知道了节的存储器地址和符号的运行时地址，计算方式如下：</p>
<pre><code>计算swap在节中的地址 refptr ＝ s＋r.offset
计算引用swap的指令地址： refaddr = ADDR(s) + r.offset
计算偏移量  *refptr = ADDR(r.symbol) + *refptr - refaddr</code></pre>
<p>书中给的例子比较难懂，首先抛开那个 *refptr不看， 参考书3.6节的内容，在进行PC相对寻址时，目标跳转地址就是例子中swap函数的地址，而链接器需要修改的就是引用swap的指令的目标编码，这两者之间的关系为：</p>
<pre><code>目标编码 ＝ 目标跳转地址 － PC</code></pre>
<p>因此在求目标编码时，实际的目标跳转地址等于swap的地址：0x80483c8,从3.6节的例子中我们可以看出PC的值是在当前指令的地址基础上＋4，即当前指令的后面一条指令，因此实际上原式等于</p>
<pre><code>目标编码 ＝ 目标跳转地址 － （当前指令地址＋4）</code></pre>
<p>当前指令自然是引用swap的指令地址，该地址可以用节基地址加偏移的方式求得:</p>
<pre><code>引用swap的指令地址 ＝ 节基地址 ＋ 引用偏移 </code></pre>
<h4 id="绝对引用">绝对引用</h4>
<p>绝对引用理解起来就容易许多，修改引用时直接把值修改为目标跳转地址和初始值的和，CPU会直接通过该引用内的值查找对应的数据</p>
<p>按照书中的例子，bufp0需要重定位指向数组buf的首地址，又由于原本.data节中的重定位条目中的初始值为0，所以直接把buf数组的首地址赋给bufp0即可。由于重定位的对象是指针，因此该过程类似于为指针赋值的过程。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/23/CNN经典论文集/" itemprop="url">
                CNN经典论文集
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-23T20:27:23+08:00" content="2015-08-23">
            2015-08-23
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/23/CNN经典论文集/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/23/CNN经典论文集/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>这篇日志主要用来整合近几年在ImageNet竞赛中获得较高成绩的CNN网络相关论文的笔记，因为思想都是基于CNN，在实际训练网络或者网络结构上有创新的地方，因此就统一写成一篇文档了，实验结果部分就省略了，主要集中在文章亮点的地方，一共有4篇相关的论文(Alexnet, GoogleNet, VGGnet和OverFeat)，会保持长期更新。</p>
<p>caffe的example中给出了Alexnet和GoogleNet的模型，熟悉caffe的话可以直接看对应的depoly.prototxt,比较直观</p>
<h2 id="alexnet">AlexNet</h2>
<h3 id="architecture">Architecture</h3>
<p>Alexnet是ILSVRC2012的冠军，网络结构比较深但是很有规律，关于这篇论文有许多在实际train网络时候的一些比较有用的trick，AlexNet的训练过程如下：</p>
<ul>
<li>rescale the image such that the shorter side is of length 256, and crop the centeral 256*256 patch from the resulting image 先rescale后crop的方式处理图像scale的问题</li>
<li>5个卷积层3个pooling 层，且神经元的激活函数使用的是ReLu，降低计算量</li>
<li>Training on multiple GPUs，现在看来GPU并不是非常好，但是多GPU并行计算</li>
<li><p><strong><em>LRN（Local Response Normalization） Normalization</em></strong> 笔者认为这是本文的亮点，虽然ReLu不需要输入normalization但是引入LRN可以提升1.4%至1.2%的准确率，简单来说就是对ReLu层的输出进行归一化，归一化公式如下： <span class="math">\[b_{x,y}^{i} = a_{x,y}^{i}/(k+\alpha\sum_{j=max(0,i-n/2)}^{min(N-1,i+n/2)}(a_{x,y}^j)^2)\]</span> 其中i和j表示第i/j个卷积的kernel，而a表示ReLu的输出，b表示归一化后的结果</p>
注：caffe的LRN层实现中提供两种Normalization的方式：通道内归一化和通道间归一化，这个在caffe源码笔记中会详细讲解</li>
<li><p>Overlapping Pooling 传统的pooling方式将feature map分成独立的grid每个grid进行pooling，而本文中采用滑动窗口的方式，每次滑动的stride小于窗口的大小，实验证明，这样的方法能提升0.4%－0.3％的准确率</p></li>
</ul>
<p>整体网络结构直接上图：<img src="/img/CNN/AlexNet.jpeg" alt="AlexNet"></p>
<h3 id="training-details">Training Details</h3>
<h4 id="data-augmentation">Data Augmentation</h4>
<p>笔者认为这一部分内容在工业界相当有用，Deep learning需要大量的数据，而当数据集不够充足的时候如何做好数据增量是十分关键的，同时还能减少overfitting</p>
<p>文中介绍了两种方法： - generating image translations and horizontal reflections(截取部分图像和镜像翻转) - alter the intensities of the RGB channels(借助PCA改变RGB通道的像素值)</p>
<h4 id="dropout">Dropout</h4>
<p>Dropout有专门的文献介绍，简单来说就是每次Forward的时候神经元以某概率输出0，这样在BP的时候该神经元的权重也不会更改</p>
<h2 id="googlenet">GoogleNet</h2>
<p>GoogleNet以及其复杂的网络结构而闻名，ILSVRC2014的冠军，一共22层的网络看上去复杂但依旧有规律可循，文章中主要的亮点在于Inception结构</p>
<h3 id="motivation">Motivation</h3>
<p>单纯加深网络会导致过拟合问题出现，且由于90%的计算量集中在最后的全连接层，为了降低计算量，文中提出了将全连接层替换为稀疏连接层。</p>
<h3 id="architecture-1">Architecture</h3>
<p>Inception Module是构成整个GoogleNet的关键组件，其主要思想就是用密集的组件来逼近稀疏的卷积网络结构</p>
<p>Naive Inception结构如下图，假设每个Inception视作一个部件级联起来，Inception在接收上一层的结果后，用不同尺寸的卷积层(1*1,3*3,5*5)和pooling层并行计算结果，将每层的结果合并起来，最后经过一个滤波器进行分组</p>
<p>但是会带来一个问题：即使是一个合适数量的卷积，也会因为大量的滤波而变得特别expensive。经过pooling层输出的合并，最终可能会导致数量级增大不可避免。处理效率不高导致计算崩溃。</p>
<p>因此改进方案是：在需要大量计算的地方进行降维。压缩信息以聚合。1*1卷积不仅用来降维，还用来引入非线性特性。</p>
<h3 id="googlenet-1">GoogleNet</h3>
<div class="figure">
<img src="/img/CNN/googlenet.jpg" alt="GoogleNet">
<p class="caption">GoogleNet</p>
</div>
<p>比较有趣的地方是网络中有三个softmax分类器，文中作者提到对于浅层网络而言，中间网络产生的特征非常有辨识力，在这些层中增加一些额外的分类器能增加BP的梯度同时提供额外的正则化，每3个Inception module会增加一个Softmax分类器。在训练过程中，损失会根据权重叠加，而在测试时丢弃。</p>
<p>Training的时候使用了AlexNet中的trick</p>
<h3 id="insights">Insights</h3>
<ul>
<li>approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural net- works for computer vision.(虽然增加了计算量但是能显著提高分类的准确率)</li>
<li>For both classification and detection, it is expected that similar quality of result can be achieved by much more ex- pensive non-Inception-type networks of similar depth and width.(没有使用bounding box regression,说明Inception同样适用于detection和localization的task)</li>
</ul>
<h2 id="vgg-net">VGG net</h2>
<p>VGG Net是ILSCVRC2014 classification的第二名，localization的第一名，网络结构上比其他网络都要来得复杂(实验过程中卷积层最多有16个)，但是文章中对如何构造复杂网络和训练网络的描述非常详尽，相信对research会有很多帮助</p>
<h3 id="network-configuration">Network Configuration</h3>
<h3 id="training">Training</h3>
<p>网络的training借鉴了AlexNet在训练中使用的方法，同样是momentum SGD＋dropout，处理图像尺寸的方法相同，做数据增量的方法也相同，但是由于一共有5个网络需要训练，且网络深度依次递增，因此在参数初始化时作者提出：</p>
<p>最简单的网络使用随机初始化，而后面的网络中最前面的4个卷积层与最后3个全连接层参数用先前网络的参数初始化,其他层使用随机初始化，这种<strong><em>首先训练简单网络，随后使用其参数来初始化复杂网络的训练方法是一种非常合理且高效的方式</em></strong></p>
<p>值得一提的是文中对比了7*7的卷积层与三个3*3的卷积层之间的对比，相比较而言多个小的卷积层级联能在引入更多的linear rectify之外同时还能降低卷积层参数的数量。</p>
<p>整个训练过程如下：</p>
<ul>
<li>Classification
<ul>
<li>设计结构复杂度依次递增的网络</li>
<li>在Image size的处理上，作者提出single-scale和multi－scale的方法, single-scale的方法与AlexNet相同，先resize后crop，multi－scale的方法类似于随机采样，每次rescale之前在一个固定区间内（如[256,512]）选取一个scale，这样每个图像的尺寸就不同了，但是怎么处理feature长度不同的问题作者似乎没有说明</li>
<li>每次训练新的网络之前用之前网络的参数进行初始化以加快收敛速度</li>
</ul></li>
<li>Localization
<ul>
<li>Training Localization的方法与classification相同，只不过用的是Euclidean Loss</li>
</ul></li>
</ul>
<h3 id="testing">Testing</h3>
<p>在testing阶段，网络接收图像数据之前首先需要把图像rescale到固定大小，作者称之为test scale，之后首先通过fc层计算，我们把fc层看作一个卷积窗口为1*1的卷积层，得到的feature map维度与object类别相同。</p>
<p>这样做的好处是对比首先crop再通过卷积计算feature map的做法，直接通过fc层计算可以减少许多重复的卷积计算量，crop出来的图像之间必然会有重叠的部分，这些重叠部分增加了测试时卷积的计算量，但是笔者认为既然在训练时使用crop来做数据增量，测试时用crop可以增加网络输出的置信度（每个crop输出分类结果，这个结果甚至可以用来做类似bagging的训练），如果面对实时性要求比较高的情况，对整张图片应用fc层是比较可行的方法。</p>
<h2 id="overfeat">OverFeat</h2>
<p>OverFeat最出彩的地方在于使用一个CNN解决Classfication、Localization和Detection的任务，以及Sliding Window和CNN的结合，每个任务使用的神经网络不同之处只在最后的分类器，大牛Yann LeCun的文章</p>
<p>网络结构的前五层与Alexnet相同，Alexnet的网络结构已经成为feature extraction的经典结构了,如下图所示： <img src="/img/CNN/OverFeat.png" alt="OverFeat"></p>
<h3 id="multiscale-training">Multiscale training</h3>
<p>训练时大多数人都会碰到Multiscale的问题，本文中作者提出在pooling时引入偏移量的方法，具体过程如下图 <img src="/img/CNN/Pooling.png" alt="Pooling"> 按照笔者理解，原始的3*3pooling的起始位置为(0,1,2),其实就是使用stride步长为1的pooling层，之后按照原始stride进行分组，得到多个分组的pooling向量之后使用滑动窗口的方式得到feature map，重新将原先分组的结果合并成一个向量作为整张图片的feature map,无论图像的尺寸多大，使用stride为1的pooling层能最大程度上保持原始信息，并且分组之后每一个feature map向量长度保持一致。</p>
<h3 id="sliding-window">Sliding Window</h3>
<p>训练CNN时要求输入图像的大小固定，但是如果输入的图像尺寸比原先大怎么办呢？对于一个训练好的CNN来说，卷积核的大小和数量以及每一层feature map的个数是固定的，但是feature map的大小是可以改变的。</p>
<p>当测试样本和训练样本大小相同时，CNN最后一层的每一个节点分别输出一个0~1的实数，代表测试样本属于某一类的概率；当测试样本比训练样本大时，CNN最后一层每一个节点的输出为一个矩阵，矩阵中的每一个元素表示对应的图像块属于某一类的概率，其结果相当于通过滑动窗口从图像中进行采样，然后分别对采样到的图像块进行操作 <img src="/img/CNN/SlidingWindow.png" alt="Sliding"></p>
<h3 id="localization">Localization</h3>
<p>将原先classification网络的最后一层softmax替换为Bounding box回归: 最后一个pooling层输出的feature map通过两个全连接层映射到一个输出层，输出被看作一个由4维向量构成的矩阵，每个向量对应了原始图像某一块区域中的object的bounding box的位置，之后开始合并各个区域中的bounding box直到两个bounding box之间的match score足够大。</p>
<p>值得注意的是这个regression网络对于每一个object类别都会输出一个bounding box，前面一节中提到的classification的网络会输出图像分类的概率值，于是每一类的bounding box都会对应一个置信度，在合并之后网络会输出一个置信度，这个置信度就是原先所有class置信度的最大值，因此完成合并之后网络的输出是一个带有最大confidence的bounding box</p>
<h3 id="detection">Detection</h3>
<p>物体检测的方法其实就是结合了classification以及localization，但是文中没有给出具体的训练细节，笔者认为与Localization非常相似，因为直观上只要根据置信度删选Localization输出的bounding box，之后取每一个图像块中置信度最大bounding box就可以了，可以看出相对于RCNN的做法，OverFeat比较暴力一些，对每块图像区域都会输出1000类的预测结果，其计算量想必也不小</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/16/Faster RCNN论文笔记/" itemprop="url">
                Faster RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-16T17:10:30+08:00" content="2015-08-16">
            2015-08-16
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/16/Faster RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/16/Faster RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>本笔记中会多次提到Fast RCNN的架构，对Fast RCNN的概念与架构有疑问的可以参考</p>
<p><a href="http://arxiv.org/pdf/1504.08083v1.pdf" target="_blank" rel="external">Fast RCNN</a></p>
<p>或者我的Fast RCNN笔记</p>
<p><a href="http://dubur.github.io/2015/08/12/Fast%20RCNN%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" target="_blank" rel="external">Notes on Fast RCNN</a></p>
<p>MSR的研究员似乎对Fast RCNN的成果还不够满意，从RCNN到Fast RCNN，所有的detection任务都使用了selective search来提取region proposal，因此诞生了用神经网络来提取region proposal的方法，进一步提升检测速度的同时还提升了检测的准确率，然而代码现在还没有公开。</p>
<h2 id="motivation">Motivation</h2>
<ul>
<li>Fast RCNN ignores the time spent on region proposals</li>
<li>Region proposal methods used in research are implemented on the CPU</li>
<li><strong><em>The feature maps can also be used for generating region proposals</em></strong> 简单说就是提取proposal的步骤太花时间，而且作者希望尽可能用GPU完成所有的计算。</li>
</ul>
<h2 id="region-proposal-networks-rpn">Region Proposal Networks (RPN)</h2>
<h3 id="basic-notions">Basic Notions</h3>
<ul>
<li>Fully-convolutional network for generating region proposals</li>
<li>Share computation of convolutions with start-of-art object detection networks</li>
</ul>
<h3 id="architecture">Architecture</h3>
<ul>
<li>RPN与卷积层共享权重，也就是说RPN的输入就是最后一个卷积层的输出的feature map，得到输入后使用滑动窗口的方式得到更低维的向量</li>
<li>将得到的向量输入到两个并联的全连接层</li>
</ul>
<ol style="list-style-type: decimal">
<li>box-regression layer (bounding box regressor)</li>
<li>box-classification layer (objectness) 类似于Fast RCNN的RoI pooling层后面的架构，feature用来训练两个分类器，一个用来判断这个RoI是否包含object，另一个用来做bounding box回归(即给定一个anchor判断bounding box的位置)</li>
</ol>
<ul>
<li><p>每个滑动窗口中心记作一个anchor，对应9种variant，相当于对于每一个feature map用9种sliding window计算vector，后面的输出也变成9倍，这么做是为了对图像的transformation有更好的鲁棒性 <img src="/img/Faster_RCNN/RPN.png" alt="RPN"></p></li>
<li><p>Loss Function的定义与Fast RCNN类似</p></li>
</ul>
<!--<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>-->
<p><span class="math">\[ L(p\_{i},t\_{i}) = L\_{cls}(p\_{i},{p\_{i}}^{*}) + \lambda{p\_{i}}^{*}L\_{reg}(t\_{i},{t\_{i}}^{*}) \]</span></p>
<p>由两部分构成：regression的Loss和classification的Loss，通过<span class="math">\(\lambda\)</span>控制两类Loss的比重 公式中<span class="math">\(p_{i}\)</span>表示一个region中包含object的概率，<span class="math">\({p_{i}}^{*}\)</span>表示ground truth， 是一个0-1指示器，当某个anchor被标注为含有一个object时它的值才是1，否则为0, <span class="math">\(L_{cls}(p_{i},{p_{i}}^{*})\)</span>的定义方式与Fast RCNN中相同</p>
<h2 id="training-rpn">Training RPN</h2>
<ul>
<li>mini batch构成</li>
</ul>
<ol style="list-style-type: decimal">
<li>随机选取一张图片的256个anchor，正负样本为1:1</li>
<li>用高斯分布初始化权重</li>
</ol>
<ul>
<li>Trainging的四个步骤 这里引用一下原文:</li>
</ul>
<ol style="list-style-type: decimal">
<li>First, train the RPN is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task.</li>
<li>In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection network is also initialized by the ImageNet-pre-trained model.</li>
<li>In the third step, we use the detector network to initialize RPN training, but we fix the shared conv layers and only fine-tune the layers unique to RPN.</li>
<li>Now the two networks share conv layers. Finally, keeping the shared conv layers fixed, we fine-tune the fc layers of the Fast R-CNN.</li>
</ol>
<p>总体而言训练方式在训练Fast RCNN的基础上做了改进，Fast RCNN输入的region proposal由RPN提供，之后用Fast RCNN的权重重新初始化RPN的参数，做到权重共享，保持这些权重不变，只对RPN中的几层进行微调，最后再对全连接层进行微调。</p>
<p>这样做的好处显而易见，实时检测时，前面卷积层提取feature map直接送给RPN， Fast RCNN等待RPN计算proposal，用GPU计算proposal速度很快，与Fast RCNN不同的是不需要selective search，提取proposal的步骤与detection合并，在原先的卷积层与RoI pooling层之间加入了一个RPN提取proposal，之后Fast RCNN的全连接层利用RPN的输出向量进行detection</p>
<p>笔者认为从RCNN到这里的Faster RCNN，有一点bounding box regression和object classification的joint learning的味道了。</p>
<p>最后RPN结合Fast RCNN可以做到单纯用神经网络进行object detection，不依靠selective search这样的low-level feature的方法，完全依靠deep learning进行图像的理解。</p>
<p>不过有时间还是要把selective search也看一下，毕竟这篇论文有很高的引用量。</p>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/12/RCNN论文笔记/" itemprop="url">
                RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-12T16:17:30+08:00" content="2015-08-12">
            2015-08-12
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/12/RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/12/RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>RCNN本身概念比较简单，但是论文中提到了许多object detection的经典方法非常有用，这些内容绝大部分在附录中，在阅读其他论文时相信会有很大帮助。</p>
<h2 id="basic-notions">Basic Notions</h2>
<ul>
<li>Pipeline method</li>
</ul>
<ol style="list-style-type: decimal">
<li>提取独立于物体类别的region proposal</li>
<li>使用CNN做特征提取</li>
<li>调整CNN参数用于从候选的proposal中获取包含object的proposal(domain-specific fine-tuning)</li>
<li>SVM进行分类</li>
</ol>
<h2 id="rcnn-for-object-detection">RCNN for Object Detection</h2>
<h3 id="testing">Testing</h3>
<ul>
<li>Selective search 提取region proposal(非本文重点)</li>
<li>使用已经训练好的CNN(dataset: ILSVRC2012 classification caffe implementation) 提取proposal的特征，为了将输入标准化，强制将每个proposal resize到227*227</li>
<li>CNN提取的特征输入到SVM中进行分类</li>
</ul>
<h3 id="training">Training</h3>
<p>按照个人理解，为了让CNN有detection的能力，我们需要让网络具备分辨哪些proposal可能包含object，因为selective search选取的许多proposal其实没有object或者仅包含了object的一小部分。</p>
<p>因此文中加了一步对CNN参数的调整(domain-specific fine-tuning),主要目的就是筛选proposal</p>
<p>在实验部分作者也进行了对比，结果证明使用了fine-tuning比不使用要高8%的mAP</p>
<ul>
<li>Supervised pre-training</li>
</ul>
<ol style="list-style-type: decimal">
<li>CNN on ILSVRC2012 with 2.2% error rate</li>
<li>training set: images with image-level annotations, <strong><em>no bounding boxes</em></strong></li>
</ol>
<ul>
<li>Domain specific fine-tuning</li>
</ul>
<ol style="list-style-type: decimal">
<li>N+1 classification: N object classes and 1 background</li>
<li>Use warped region proposals for tuning</li>
<li>Positive training examples: region proposals with &gt;0.5IoU overlap(the rest as negative)</li>
</ol>
<ul>
<li>Object category classification</li>
</ul>
<ol style="list-style-type: decimal">
<li>Positive examples: ground-truth boxes, Negative: region proposals with &lt;0.3IoU overlap</li>
<li>Trade-off between SVM and Softmax</li>
<li>Different definition for training sets between fine-tuning and classification</li>
</ol>
<p>这里作者提了两个问题 (在附录里均有解答):</p>
<ol style="list-style-type: decimal">
<li>为什么在第二步调整CNN参数使用的训练集和训练SVM分类器的训练集不同(尤其是threshold的选择不一样)?</li>
<li>为什么不使用softmax分类器而选择SVM?</li>
</ol>
<h2 id="appendix">Appendix</h2>
<ul>
<li>Object proposal transformations 由于CNN输入是227*227的正方形图像，因此需要对proposal进行缩放处理，文中提出了3种不同的处理方式：</li>
</ul>
<ol style="list-style-type: decimal">
<li>tightest square with context(等比例缩放proposal在原始图像中的bounding square)</li>
<li>tightest square without context(等比例缩放proposal在原始图像中的bounding square,但要去掉在bounding square中而不在原始proposal中的部分)</li>
<li>强制缩放proposal 3种变形的方式如图中所示:<img src="/img/RCNN/transformation.png" alt="transformation"></li>
</ol>
<ol style="list-style-type: upper-alpha">
<li>origin proposal (B) tightest square with context (C) tightest square without context (D) warp</li>
</ol>
<ul>
<li>Positive vs negative examples and softmax</li>
</ul>
<ol style="list-style-type: decimal">
<li>用于fine－tuning的数据集过少，因此将降低了positive example的要求，只要IoU大于0.5就认为可能有object在这个proposal里面，作者同时指出这样能够防止overfitting</li>
<li>使用softmax比SVM降低3.3%的mAP，主要原因可能在于fine-tuning的训练集不那么强调box的位置，且SVM使用的nagative sample更具有区分度</li>
</ol>
<ul>
<li>Bounding-box 回归</li>
</ul>
<ol style="list-style-type: decimal">
<li>使用最后一个pool层的输出训练线性回归模型来预测detection window</li>
<li>实验结果表明使用BB regression效果更优</li>
</ol>
<h2 id="insights">Insights</h2>
<ul>
<li>Most feature information is extracted by the convolutional layers</li>
<li>The structure of CNN really matters(nearly 7~8% mAP)</li>
</ul>
<p>下面两点是这篇文章的核心：</p>
<ul>
<li>High-capacity CNN to bottom-up region proposals in order to localize and segment objects(CNN用于detection)</li>
<li>Effective method: supervised pretraining (auxiliary data eg. classification)/fine-tuning(scarce data eg. detection)(fine-tuning的重要性不言而喻)</li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/12/Fast RCNN论文笔记/" itemprop="url">
                Fast RCNN论文笔记
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-12T16:17:30+08:00" content="2015-08-12">
            2015-08-12
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                  <span itemprop="name">Deep Learning</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/12/Fast RCNN论文笔记/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/12/Fast RCNN论文笔记/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>Fast RCNN在RCNN的基础上修改了网络结构，同时加快了训练速度并且降低了训练成本，从工程角度说RCNN提出了思路而Fast RCNN的实用性更强</p>
<h2 id="motivation-drawbacks-of-rcnn">Motivation (Drawbacks of RCNN)</h2>
<ul>
<li>RCNN的训练方式按照一个一个阶段分的比较开</li>
<li>训练时RCNN会将每个proposal的特征向量存储在硬盘上，不仅占用存储空间，时间上也因为读写磁盘而变得很慢</li>
<li>在进行detection测试时耗时长</li>
</ul>
<h2 id="fast-rcnn-training">Fast RCNN Training</h2>
<p>文章的亮点在于对原来网络结构的改进，这里就不得不膜拜大神了</p>
<ul>
<li>RoI Pooling Layer 按照作者所述，该层的思想类似于SPPnet，我们假设最后一个卷积层得到输入N个feature map(这些feature map包含原始图像的所有信息)，而我们只想提取感兴趣的部分，因此这就需要引入RoI了</li>
</ul>
<p>将这N个feature map和R个region proposal输入到RoI pooling层中，根据RoI中指定的区域对原始的feature map进行pooling得到这R个region proposal的N个feature map</p>
<p>笔者觉得其实和SPPnet没有多大的关系……由于各个RoI的大小不一样，而为了使得每个RoI pooling出来的feature长度相同，采用不同大小的pooling窗口而已</p>
<ul>
<li>对原始CNN网络结构的修改 修改主要有3处</li>
</ul>
<ol style="list-style-type: decimal">
<li>将最后一个max pooling层替换为RoI pooling层</li>
<li>将最后一个全连接层和softmax层替换为两个同并列的层
<ol style="list-style-type: decimal">
<li>K+1个class的分类器</li>
<li>Bounding box回归(Localization)</li>
</ol></li>
<li>修改网络的输入使其接收N个image和R个RoI 最后网络结构如图所示：<img src="/img/Fast_RCNN/fast-rcnn.jpeg" alt="fast-rcnn"></li>
</ol>
<p>核心在于使用RoI pooling层直接提取原始feature map中属于RoI的那一部分</p>
<ul>
<li><p>Fine-tuning微调</p>
<p>网络结构变化带来的是训练方式的变化，作者在现有的CNN分类网络上进行了微调，类似于RCNN中的domain-specific fine-tuning,只不过这次是针对网络结构而调整训练方式</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Multi-task Loss</p>
<p>简单来说就是将分类的Loss和bounding box回归的Loss做了加权和</p>
<p><span class="math">\[ L(p,k^*,t,t^*) = L_{cls}(p,k^*) + \lambda[k^*\ &gt;\ 1]L_{loc}(t,t^*) \]</span></p>
<p><span class="math">\[ smooth_{L_1}(x) = \begin{cases}0.5x^2 \ &amp;if \ |x| &lt; 1
 \cr |x|-0.5 \ &amp;otherwise \end{cases} \]</span></p>
<p>第一部分是softmax分类器的Loss，第二部分是regression的Loss，值得注意的是第二部分有个指示函数，说明不会计算那些background类型的proposal的locaolization误差</p></li>
<li><p>mini-batch sampling</p>
<p>每一次SGD的batch使用2个image和128个RoI，每个batch中RoI的构成: 1.25% ground truth中的bounding box IoU&gt;0.5, 2.75% IoU &lt; 0.5, 看作background example</p></li>
<li><p>BP through RoI pooling layer</p>
<p>在进行反向传播时RoI pooling layer有些许不同, 参与pooling的不再是整幅图像而是可能有重叠的RoI区域，计算公式如下:</p>
<p><span class="math">\[ {\partial L \over \partial x} = \sum_{r\in R} \sum_{y\in r} [y\ pooled\ x] {\partial L \over \partial y} \]</span></p>
<p>y代表pooling后的值, 反向传播的思想是要将每个y的偏导传递给所有参与pooling的变量x，这也就是指示函数 [y pooled x] 的含义，在这里x代表的是原来RoI中的元素</p></li>
</ol>
<ul>
<li>Scale invariance 如何处理输入图像尺寸不一的问题？ 作者给出了两种方案:</li>
<li>brute force 强制把图像resize到同一尺寸下</li>
<li>image pyramids 利用图像金字塔，把图片resize成不同的大小，选取其中最接近optimal scale（也就是227*227）的大小用作训练。</li>
</ul>
<h2 id="fast-rcnn-detection">Fast RCNN Detection</h2>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<ul>
<li>Truncated SVD 全连接层的计算时间太长，占用了整个前向传播的一半时间，因此在进行矩阵乘法时，作者采用SVD将权重矩阵<span class="math">\(W (u \times v)\)</span>分解为三个矩阵的乘积：</li>
</ul>
<p><span class="math">\[ W \approx U \Sigma_tV^T \]</span></p>
<p>SVD分解后的U和V都是对称矩阵 最后一个全连接层被分解为两个子层:第一层的权重矩阵为$ _tV^T <span class="math">\(,第二层的权重矩阵为\)</span>U<span class="math">\(,因此整个参数的个数由\)</span>uv<span class="math">\(变成了\)</span>t(u+v)$,当t比较小(意味着原参数矩阵的特征值少)的时候计算时间就会大大减少 <img src="/img/Fast_RCNN/training_time.png" alt="training_time"></p>
<h2 id="experiment-insights">Experiment Insights</h2>
<ul>
<li>卷积层和全连接层一样需要fine-tuning &gt; decrease mAP from 66.9% to 61.4%</li>
</ul>
<p>且浅层的卷积层没有微调的必要。</p>
<ul>
<li><p>首先用classification loss训练，之后加入bbox regression训练效果更好 &gt; improvement ranges from +0.8 to +1.1 mAP points</p></li>
<li>trade-off: speed &gt; performance improvement brought by multi-scale</li>
<li>More data brings higher mAP</li>
<li>Softmax slightly outperforms SVM</li>
<li><p>Sparse proposals(selective search) improve detection quality and dense proposals(sliding window) free the heavy running cost.</p></li>
</ul>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <header class="post-header">

      
      
        <h1 class="post-title" itemprop="name headline">
          
          
            
              <a class="post-title-link" href="/2015/08/06/theano-and-caffe-installation/" itemprop="url">
                Ubuntu14.04 安装 theano 和 caffe
              </a>
            
          
        </h1>
      

      <div class="post-meta">
        <span class="post-time">
          發表於
          <time itemprop="dateCreated" datetime="2015-08-06T23:34:01+08:00" content="2015-08-06">
            2015-08-06
          </time>
        </span>

        
          <span class="post-category" >
            &nbsp; | &nbsp; 分類於
            
              <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                <a href="/categories/Instructions/" itemprop="url" rel="index">
                  <span itemprop="name">Instructions</span>
                </a>
              </span>

              
              

            
          </span>
        

        
          
            <span class="post-comments-count">
              &nbsp; | &nbsp;
              <a href="/2015/08/06/theano-and-caffe-installation/#comments" itemprop="discussionUrl">
                <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/06/theano-and-caffe-installation/" itemprop="commentsCount"></span>
              </a>
            </span>
          
        
      </div>
    </header>

    <div class="post-body">

      
      

      
        
          <span itemprop="articleBody"><p>说明:安装时为了支持python版本，且为了安装方便，最好首先安装anaconda，anaconda安装起来方便并且集成了许多科学计算库，主要为了配合theano使用</p>
<p>如果想安装opencv、添加caffe matlab等等最好先看教程中相关部分最后再编译caffe比较保险</p>
<h2 id="安装caffe">安装caffe</h2>
<h3 id="安装build-essentials">1.安装build-essentials</h3>
<p>安装开发所需要的基本包（一般装完系统就有） sudo apt-get install build-essential</p>
<h3 id="安装nvidia驱动">2.安装NVIDIA驱动</h3>
<h4 id="退出图形界面">2.1 退出图形界面</h4>
<pre><code>1）由于有的带有gpu的电脑在启动时默认使用独显作为主要显示设备，因此我们需要将bios设置改为使用集显作为显示设备
2）进入ubuntu，按ctrl＋alt＋F1进入tty，登陆tty后输入：
    sudo service lightdm stop
其他desktop manager也需要关闭</code></pre>
<h4 id="安装驱动">2.2安装驱动</h4>
<p>准备工作：</p>
<p>1）Verify the system has a CUDA-capable GPU.</p>
<p>控制台输入以下命令：</p>
<pre><code>lspci | grep -i nvidia</code></pre>
<p>如果列出了当前NIVDIA显卡的信息则说明电脑的GPU事CUDA-capable的</p>
<p>2）Verify the system is running a supported version of Linux.</p>
<p>控制台输入以下命令:</p>
<pre><code>uname -m &amp;&amp; cat /etc/*release</code></pre>
<p>输出的结果中如果显示x86_64则说明电脑是x86架构的，在下载驱动安装包时选择对应的包即可</p>
<p>3）Verify the system has gcc installed.</p>
<p>控制台输入:</p>
<pre><code>gcc --version</code></pre>
<p>确认gcc版本正确</p>
<p>4）Download the NVIDIA CUDA Toolkit.</p>
<p>从NVIDIA官网上下载对应的驱动安装包 下载地址:</p>
<pre><code> https://developer.nvidia.com/cuda-downloads</code></pre>
<p>控制台输入以下命令：</p>
<pre><code> sudo dpkg -i cuda-repo-&lt;distro&gt;_&lt;version&gt;_&lt;architecture&gt;.deb
        sudo apt-get update
        sudo apt-get install cuda</code></pre>
<p>5)配置环境变量和lib库路径 安装完成后需要在/etc/profile中添加环境变量, 在文件最后添加:</p>
<pre><code>PATH=/usr/local/cuda-7.0/bin:$PATH
export PATH
    </code></pre>
<p>保存后使得环境变量立刻生效(或者重新打开控制台)</p>
<pre><code>source /etc/profile
    </code></pre>
<p>在 /etc/ld.so.conf.d/加入文件 cuda.conf, 内容如下</p>
<pre><code>/usr/local/cuda-7.0/lib64</code></pre>
<p>6）验证是否安装正确:</p>
<pre><code>cd /usr/local/cuda-7.0/bin/
cuda-install-samples-7.0.sh &lt;dir&gt; 
    </code></pre>
<p>dir为目标路径，脚本会把sample文件复制到指定路径下,之后进入该路径make就好了</p>
<p>编译完成后进入bin/x86_64/linux/release</p>
<p>执行./deviceQuery 如显示GPU信息说明cuda安装正确</p>
<h3 id="安装atlas">3 安装atlas</h3>
<p>使用gpu加速需要安装blas库，这里选择atlas</p>
<pre><code>sudo apt-get install libatlas-base-dev</code></pre>
<h3 id="下载caffe安装包与安装环境">4 下载caffe安装包与安装环境</h3>
<p>控制台输入:</p>
<pre><code>git clone https://github.com/BVLC/caffe</code></pre>
<p>也可以到github上下载zip文件解压</p>
<p>安装caffe的依赖库:</p>
<pre><code>sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev
sudo apt-get install --no-install-recommends libboost-all-dev
sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler</code></pre>
<h3 id="安装anaconda">5 安装anaconda</h3>
<p>不使用ubuntu自带的python是因为anaconda集成了许多科学计算库如numpy、scipy等，而且安装theano也很方便 在anaconda官网上下载安装包直接安装即可，默认安装路径为</p>
<pre><code>~/anaconda</code></pre>
<p>添加bin到PATH环境变量即可 打开控制台输入python确认使用的是anaconda里面的python 安装pip</p>
<pre><code>conda install pip</code></pre>
<h3 id="安装caffe所需要的python环境">6 安装Caffe所需要的Python环境</h3>
<p>然后执行如下命令安装编译caffe python wrapper 所需要的额外包</p>
<pre><code>cd python
for req in $(cat requirements.txt); do sudo pip install $req; done</code></pre>
<p>在运行Caffe时，可能会报一些找不到libxxx.so的错误，而用 locate libxxx.so命令发现已经安装在anaconda中，这时首先想到的是在/etc/ld.so.conf.d/ 下面将 $your_anaconda_path/lib 加入 LD_LIBRARY_PATH中。 但是这样做可能导致登出后无法再进入桌面！！！原因（猜测）可能是anaconda的lib中有些内容于系统自带的lib产生冲突。</p>
<p>正确的做法是：为了不让系统在启动时就将anaconda/lib加入系统库目录，可以在用户自己的~/.bashrc 中添加library path，比如我就在最后添加了两行</p>
<pre><code># add library path
LD_LIBRARY_PATH=your_anaconda_path/lib:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH</code></pre>
<p>开启另一个终端后即生效，并且重启后能够顺利加载lightdm, 进入桌面环境。</p>
<p>但在实际安装时注意先要等make和make test完之后再加这个路径否则会报错</p>
<h3 id="编译caffe">7 编译caffe</h3>
<p>完成了所有环境的配置，可以愉快的编译Caffe了！ 进入caffe根目录， 首先复制一份Makefile.config</p>
<pre><code>cp Makefile.config.example Makefile.config</code></pre>
<p>然后修改里面的内容，主要需要修改的参数包括</p>
<p>BLAS (默认使用的是atlas)</p>
<p>DEBUG 是否使用debug模式，打开此选项则可以在eclipse或者NSight中debug程序</p>
<p>如要安装python caffe和mat caffe的话需要在Makefile.config中指定python路径和matlab路径</p>
<p>完成设置后， 开始编译</p>
<pre><code>make all
make test
make runtest</code></pre>
<p>前两步成功说明caffe编译成功，make runtest是在运行caffe的各个测试脚本</p>
<h3 id="编译pycaffe">8 编译pycaffe</h3>
<p>控制台输入: make pycaffe</p>
<p>之后设置环境变量 在～/.bashrc中添加</p>
<pre><code>PYTHONPATH=/path/tp/caffe/python:$PYTHONPATH
export PYTHONPATH</code></pre>
<p>打开控制台输入python，之后import caffe，import成功就说明caffe安装成功了</p>
<h3 id="安装cudnn可选">9 安装cudnn（可选）</h3>
<h4 id="编译caffe之后安装cudnn">9.1编译caffe之后安装cudnn</h4>
<p>使用cudnn来加速GPU计算，cudnn支持caffe，theano和Torch7 在官网免费获得cudnn压缩包 下载之后控制台输入:</p>
<pre><code>tar -xzvf cudnn-6.5-linux-R1.tgz
cd cudnn-6.5-linux-R1
sudo cp lib* /usr/local/cuda/lib64/
sudo cp cudnn.h /usr/local/cuda/include/</code></pre>
<p>之后建立软链接（首先删除原先文件夹下的软链接）:</p>
<pre><code>cd /usr/local/cuda/lib64/
sudo rm -rf libcudnn.so libcudnn.so.6.5</code></pre>
<p>然后修改文件权限，并创建新的软连接:</p>
<pre><code>sudo chmod u=rwx,g=rx,o=rx libcudnn.so.6.5.18（未必是18，我的机子上是48，相应改就可以了） 
sudo ln -s libcudnn.so.6.5.18 libcudnn.so.6.5
sudo ln -s libcudnn.so.6.5 libcudnn.so</code></pre>
<h4 id="编译caffe之前安装cudnn">9.2编译caffe之前安装cudnn</h4>
<p>解压cudnn之后只需将对应的文件复制到对应目录中即可，同时在编译caffe时Makefile.config中</p>
<pre><code>use_cudnn:=1</code></pre>
<p>去掉注释</p>
<pre><code>unpack the library  
gzip -d cudnn-6.5-linux-x64-v2.tar.gz  
tar xf cudnn-6.5-linux-x64-v2.tar  

copy the library files into CUDA&#39;s include and lib folders  
sudo cp cudnn-6.5-linux-x64-v2/cudnn.h /usr/local/  cuda-7.0/include  
sudo cp cudnn-6.5-linux-x64-v2/libcudnn* /usr/local/    cuda-7.0/lib64      </code></pre>
<h3 id="安装opencv可选">10 安装opencv(可选)</h3>
<p>opencv库在运行其他开源项目时可能需要用到因此也建议安装，这里安装的是支持GPU的，因此安装是需要添加支持GPU的选项</p>
<p>opencv安装起来神烦，Github上有人已经写好了完整的安装脚本： https://github.com/jayrambhia/Install-OpenCV</p>
<p>下载该脚本，进入Ubuntu/2.4 目录, 给所有shell脚本加上可执行权限： chmod +x *.sh</p>
<p>安装2.4.9版本: sudo ./opencv2_4_9.sh</p>
<p>该脚本会去尝试下载2.4.9的压缩包，如果下载速度太慢建议还是自己先下载好，再把脚本中下载的语句注释掉，sh文件需要修改的是在cmake的那一行中添加一个编译参数 -D BUILD_TIFF=ON，否则在make caffe的时候会报类似</p>
<pre><code>/usr/lib/libopencv_highgui.so.2.4: undefined reference to TIFFRGBAImageOK@LIBTIFF_4.0&#39; 1&gt; </code></pre>
<p>的错误</p>
<p>安装过程中可能会遇到两个问题:</p>
<ul>
<li><p>1编译过程中报错：</p>
<pre><code>opencv-2.4.9/modules/gpu/src/nvidia/core/NCVPixelOperations.hpp(51): error: a storage class is not allowed in an explicit specialization</code></pre>
<p>http://www.samontab.com/web/2014/06/installing-opencv-2-4-9-in-ubuntu-14-04-lts/</p></li>
</ul>
<p>解决方法在此：http://code.opencv.org/issues/3814 下载 NCVPixelOperations.hpp 替换掉opencv2.4.9内的文件， 重新build</p>
<ul>
<li><p>2编译过程中报错：</p>
<pre><code>nvcc fatal: Unsupported gpu architecture: &#39;compute xx&#39;</code></pre>
<p>需要在cmake时添加参数指定你的GPU架构，参考这两篇blog:</p>
<p>http://blog.csdn.net/sysuwuhongpeng/article/details/45485719 http://blog.csdn.net/altenli/article/details/44199539</p></li>
</ul>
<h3 id="caffe-matlab可选">11 caffe matlab(可选)</h3>
<h4 id="安装matlab">1 安装matlab：</h4>
<p>参考教程：</p>
<p>Caffe提供了MATLAB接口， 有需要用MATLAB的同学可以额外安装MATLAB。安装教程请自行搜索。</p>
<p>安装完成后添加图标 matlab图标在该网站可以下载</p>
<pre><code>http://www.linuxidc.com/Linux/2011-01/31632.htm</code></pre>
<p>图标放到/usr/local/MATLAB/下</p>
<p>控制台输入以下内容</p>
<pre><code>sudo vi /usr/share/applications/Matlab.desktop</code></pre>
<p>复制代码</p>
<pre><code>[Desktop Entry]
Type=Application
Name=Matlab
GenericName=Matlab R2014a
Comment=Matlab:The Language of Technical Computing
Exec=sh /usr/local/MATLAB/R2014a/bin/matlab -desktop
Icon=/usr/local/MATLAB/Matlab.png
Terminal=false
Categories=Development;Matlab;</code></pre>
<h4 id="matlab-wrapper">2 matlab wrapper</h4>
<p>控制台输入：</p>
<pre><code>make matcaffe</code></pre>
<p>安装完后在运行matlab demo时遇到报错</p>
<pre><code>“libhdf5.so.6 no such file or directory”</code></pre>
<p>但是到anaconda的lib目录下发现是有对应文件的，可能是动态链接库的问题，解决方法是到anaconda/lib目录下执行ldconfig就可以了</p>
<h2 id="安装theano">安装theano</h2>
<h3 id="安装theano-1">1 安装theano</h3>
<p>控制台输入 pip install theano即可</p>
<h3 id="配置theano使其支持gpu加速">2 配置theano使其支持gpu加速</h3>
<p>参考文档： http://deeplearning.net/software/theano/tutorial/using_gpu.html http://deeplearning.net/software/theano/install.html#gpu-linux http://deeplearning.net/software/theano/library/config.html#config.init_gpu_device</p>
<p>在用户根目录下新建.theanorc配置文档，配置如下：</p>
<pre><code>[global]
floatX = float32
device = gpu

[nvcc]
fastmath = True

[blas]
ldflags = -lf77blas -latlas -lgfortran #put your flags here</code></pre>
<p>检验是否theano可以使用gpu加速，测试脚本如下：</p>
<pre><code>from theano import function, config, shared, tensor, sandbox
import numpy
import time

vlen = 10 * 30 * 768  # 10 x #cores x # threads per core
iters = 1000

rng = numpy.random.RandomState(22)
x = shared(numpy.asarray(rng.rand(vlen), config.floatX))
f = function([], tensor.exp(x))
print f.maker.fgraph.toposort()
t0 = time.time()
for i in xrange(iters):
    r = f()
t1 = time.time()
print &#39;Looping %d times took&#39; % iters, t1 - t0, &#39;seconds&#39;
print &#39;Result is&#39;, r
if numpy.any([isinstance(x.op, tensor.Elemwise) and
              (&#39;Gpu&#39; not in type(x.op).__name__)
              for x in f.maker.fgraph.toposort()]):
    print &#39;Used the cpu&#39;
else:
    print &#39;Used the gpu&#39;
如果显示used the gpu说明theano在使用gpu加速</code></pre>
</span>
        
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
 </div>

        

        
      </div>

      
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="John Yao" itemprop="image"/>
          <p class="site-author-name" itemprop="name">John Yao</p>
        </div>
        <p class="site-description motion-element" itemprop="description">Dubers Blog</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">10</span>
              <span class="site-state-item-name">文章</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">5</span>
              <span class="site-state-item-name">分類</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">7</span>
              <span class="site-state-item-name">標籤</span>
              </a>
          </div>

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy; &nbsp; 
  <span itemprop="copyrightYear">2015</span>
  <span class="with-love">
    <i class="icon-next-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Yao</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 強力驅動
</div>

<div class="theme-info">
  主題 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"dubur"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     
  	<script src="/js/ua-parser.min.js"></script>
  	<script src="/js/hook-duoshuo.js"></script>
  

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.1"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.1"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.1" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.1"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  

  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }
    });
  </script>

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
